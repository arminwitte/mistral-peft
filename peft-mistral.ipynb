{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Domain Adaptation using QLoRA\n\nThis notebook demonstrates how to:\n1. Extract text from a technical PDF\n2. Prepare training data for causal language modelling (CLM)\n3. Fine-tune a language (llama 3.2 3b) base model using QLoRA\n4. Apply the QLoRA and the learned weights to the instruct model\n5. Answer some test questions","metadata":{}},{"cell_type":"code","source":"# Clone the git repo to access the utilities\n!git clone https://github.com/arminwitte/mistral-peft mistralpeft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:17:35.468282Z","iopub.execute_input":"2025-02-24T21:17:35.468580Z","iopub.status.idle":"2025-02-24T21:17:58.472384Z","shell.execute_reply.started":"2025-02-24T21:17:35.468549Z","shell.execute_reply":"2025-02-24T21:17:58.471522Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'mistralpeft'...\nremote: Enumerating objects: 204, done.\u001b[K\nremote: Counting objects: 100% (21/21), done.\u001b[K\nremote: Compressing objects: 100% (19/19), done.\u001b[K\nremote: Total 204 (delta 4), reused 14 (delta 2), pack-reused 183 (from 3)\u001b[K\nReceiving objects: 100% (204/204), 641.52 MiB | 42.42 MiB/s, done.\nResolving deltas: 100% (98/98), done.\nUpdating files: 100% (62/62), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Make sure to be on the repo directory and pull\nimport os\nif not os.getcwd() == \"/kaggle/working/mistralpeft\":\n    os.chdir(\"/kaggle/working/mistralpeft\")\n!pwd\n!git fetch --all\n!git reset --hard origin/main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:17:58.473349Z","iopub.execute_input":"2025-02-24T21:17:58.473679Z","iopub.status.idle":"2025-02-24T21:17:59.339487Z","shell.execute_reply.started":"2025-02-24T21:17:58.473649Z","shell.execute_reply":"2025-02-24T21:17:59.338623Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"/kaggle/working/mistralpeft\nFetching origin\nHEAD is now at 0224574 Merge branch 'main' of https://github.com/arminwitte/mistral-peft\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install the required packages from pypi\n!pip install -r requirements.txt --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:17:59.340423Z","iopub.execute_input":"2025-02-24T21:17:59.340699Z","iopub.status.idle":"2025-02-24T21:18:06.929136Z","shell.execute_reply.started":"2025-02-24T21:17:59.340675Z","shell.execute_reply":"2025-02-24T21:18:06.928269Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load packages\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer, pipeline\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PeftModel\nfrom datasets import Dataset\n\nfrom mistralpeft.utils import TextExtractor, CLMPreprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:06.930921Z","iopub.execute_input":"2025-02-24T21:18:06.931135Z","iopub.status.idle":"2025-02-24T21:18:30.143552Z","shell.execute_reply.started":"2025-02-24T21:18:06.931116Z","shell.execute_reply":"2025-02-24T21:18:30.142897Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Login to HuggingFace using Kaggle's secrets to be able to download models\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"huggingface\")\nlogin(secret_value_0) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:30.145042Z","iopub.execute_input":"2025-02-24T21:18:30.145733Z","iopub.status.idle":"2025-02-24T21:18:30.326716Z","shell.execute_reply.started":"2025-02-24T21:18:30.145708Z","shell.execute_reply":"2025-02-24T21:18:30.325859Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 1. Extract Sentences from PDF\nSeveral PDFs from my former research group at university (Thermo-Fluiddynamics Group, Prof. Polifke) are chosen to form the corpus","metadata":{}},{"cell_type":"code","source":"# TextExtractor is a simple ETL class to acquire a text corpus\npdf_files = [\n    \"Dissertation.pdf\",\n]\n    \npdf_urls = [\n    \"https://mediatum.ub.tum.de/doc/1360567/1360567.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1601190/1601190.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1597610/1597610.pdf\"\n    \"https://mediatum.ub.tum.de/doc/1584750/1584750.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1484812/1484812.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1335646/1335646.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1326486/1326486.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1306410/1306410.pdf\",\n    \"https://mediatum.ub.tum.de/doc/1444929/1444929.pdf\",\n]\n\ndata_path = Path(\"data/processed_documents.json\")\nif not data_path.is_file():\n    with TextExtractor(\"data/processed_documents.json\") as extractor:\n        # Process local files\n        extractor.process_documents(pdf_files)\n            \n        # Process URLs\n        extractor.process_documents(pdf_urls, url_list=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:30.327694Z","iopub.execute_input":"2025-02-24T21:18:30.327999Z","iopub.status.idle":"2025-02-24T21:18:30.332566Z","shell.execute_reply.started":"2025-02-24T21:18:30.327971Z","shell.execute_reply":"2025-02-24T21:18:30.331726Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 2. Prepare MCLM Training Data","metadata":{}},{"cell_type":"code","source":"# Specify the model and load the tokenizer\n# Llama 3.2 3B with approx. 3 billion parameters\nmodel_name = \"mistralai/Mistral-7B-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:30.333300Z","iopub.execute_input":"2025-02-24T21:18:30.333493Z","iopub.status.idle":"2025-02-24T21:18:31.683955Z","shell.execute_reply.started":"2025-02-24T21:18:30.333477Z","shell.execute_reply":"2025-02-24T21:18:31.683258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"617fe81e7e134a47ad37d3c5dac1fec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63538cfc251d41eaa863e0165408a71a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04424be09e6740cdb853f006e6ce22d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bcd9610a3f240e0bc3f1aaae2e6dd93"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Preprocess the corpus for Causal Language Modeling (CLM)\njson_file_paths = [\"data/processed_documents.json\"]\npreprocessor = CLMPreprocessor(json_file_paths, tokenizer)\ndataset = preprocessor.preprocess()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:31.684810Z","iopub.execute_input":"2025-02-24T21:18:31.685045Z","iopub.status.idle":"2025-02-24T21:18:35.417077Z","shell.execute_reply.started":"2025-02-24T21:18:31.685025Z","shell.execute_reply":"2025-02-24T21:18:35.416199Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Split into training and test set\ntrain_test_set = dataset.train_test_split(test_size=0.1)\nprint(f\"Created {len(train_test_set['train'])} training examples and {len(train_test_set['test'])} test examples\")\n\n# Preview a training example\nexample = train_test_set[\"train\"][0]\nprint(\"\\nExample input:\")\nprint(preprocessor.tokenizer.decode(example['input_ids'][:256]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:35.418008Z","iopub.execute_input":"2025-02-24T21:18:35.418246Z","iopub.status.idle":"2025-02-24T21:18:35.446544Z","shell.execute_reply.started":"2025-02-24T21:18:35.418226Z","shell.execute_reply":"2025-02-24T21:18:35.445896Z"}},"outputs":[{"name":"stdout","text":"Created 264 training examples and 30 test examples\n\nExample input:\neffort should be spent to overcome these limitations. As the computational effort of the hybrid models is still considerable, methods to build nonlinear low-order models of the ﬂame dynamics in a general and con- sistent way are required. In the scope of this thesis, in P APER -ANN, artiﬁcial neural networks have been used to extend the CFD/SI approach to the nonlin- ear regime. Unfortunately, a high uncertainty of amplitudes of thermoacoustic oscillations predicted was observed. Hence, more sophisticated methods are re- quired. One way to improve the results are white- or grey-box models, which account for the physics of the ﬂame more accurately. Another idea is to use not only the time series of the input and output signal to identify the model, but also 12 Hybrid Reduced Order / LES Models of self-e Xcited Combustion Instabilities in Multi-Burner Systems ﬁeld data. This allows to use more information to build the models which should reduce the length of the time series required. These models should be validated in the systematic procedure proposed in PAPER -ANN. Bibliography W. Polifke. “\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 3. Load and Prepare Model","metadata":{}},{"cell_type":"code","source":"# Load the base model\n# Q4_K_M quantization of the base model is achieved through BitsAndBytes. It requires CUDA!\nquantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # Use load_in_4bit=True for 4-bit quantization\n        bnb_4bit_quant_type=\"nf4\", # use normalized float 4\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=False, # do not quantize scaling factors for Q4_K_M\n    )\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    quantization_config=quantization_config,\n    )\nif base_model.config.pad_token_id is None:\n    base_model.config.pad_token_id = base_model.config.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:18:35.447453Z","iopub.execute_input":"2025-02-24T21:18:35.447795Z","iopub.status.idle":"2025-02-24T21:25:10.516424Z","shell.execute_reply.started":"2025-02-24T21:18:35.447765Z","shell.execute_reply":"2025-02-24T21:25:10.515549Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd19c98ed1dd48d9802938700dfa5adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b24c8e7dd647e98acb87e8df67d927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824336463a954c419f1c584168f2f669"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6161637a6e3241fa820b0338084e71e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab4f0ffe2874b25aed3f889889e0b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef489bae86442afb912dd3e68c5aa7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06e4a7b5b6f7414db0162bce51638706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d6834e1c71404eab38a5c41569572a"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Configure the (Q)LoRA adaptor to use a rank of r=4\nmodel = prepare_model_for_kbit_training(base_model)\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",],# \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\npeft_model = get_peft_model(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:10.517421Z","iopub.execute_input":"2025-02-24T21:25:10.517670Z","iopub.status.idle":"2025-02-24T21:25:10.699597Z","shell.execute_reply.started":"2025-02-24T21:25:10.517650Z","shell.execute_reply":"2025-02-24T21:25:10.698689Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 4. Train the Model\n\nThe LoRA adapter has about 6M parameters to train (compared to 3B parameters of the full LLM)","metadata":{}},{"cell_type":"code","source":"# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4, # Creates a virtual batch size of 3\n    learning_rate=1e-4,\n    fp16=True, # numerical precision of adapter is float16\n    logging_steps=1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    optim=\"paged_adamw_8bit\", # Memory efficient optimizer\n    log_level=\"info\",\n    report_to=\"none\",\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_test_set['train'],\n    eval_dataset=train_test_set['test']\n)\n\n# Start training\ntrainer.train(resume_from_checkpoint=\"results/checkpoint-66\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:10.700554Z","iopub.execute_input":"2025-02-24T21:25:10.700878Z","iopub.status.idle":"2025-02-24T21:25:11.544880Z","shell.execute_reply.started":"2025-02-24T21:25:10.700856Z","shell.execute_reply":"2025-02-24T21:25:11.544165Z"}},"outputs":[{"name":"stderr","text":"Using auto half precision backend\nLoading model from results/checkpoint-66.\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n***** Running training *****\n  Num examples = 264\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 66\n  Number of trainable parameters = 4,718,592\n  Continuing training from checkpoint, will skip to saved global_step\n  Continuing training from epoch 1\n  Continuing training from global step 66\n  Will skip the first 1 epochs then the first 0 batches in the first epoch.\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from ./results/checkpoint-66 (score: 3.9107666015625).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [66/66 : < :, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=66, training_loss=0.0, metrics={'train_runtime': 0.0611, 'train_samples_per_second': 4319.656, 'train_steps_per_second': 1079.914, 'total_flos': 4.618544199657062e+16, 'train_loss': 0.0, 'epoch': 1.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Save the LoRA adapter weights:\nlora_save_path = \"lora_weights\" \npeft_model.save_pretrained(lora_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:11.547293Z","iopub.execute_input":"2025-02-24T21:25:11.547518Z","iopub.status.idle":"2025-02-24T21:25:11.717599Z","shell.execute_reply.started":"2025-02-24T21:25:11.547499Z","shell.execute_reply":"2025-02-24T21:25:11.716652Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/snapshots/d8cadc02ac76bd617a919d50b092e59d2d110aff/config.json\nModel config MistralConfig {\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32768\n}\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 6. Test the Model","metadata":{}},{"cell_type":"markdown","source":"peft_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-231\")","metadata":{}},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(base_model, lora_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:11.719195Z","iopub.execute_input":"2025-02-24T21:25:11.719578Z","iopub.status.idle":"2025-02-24T21:25:11.899393Z","shell.execute_reply.started":"2025-02-24T21:25:11.719540Z","shell.execute_reply":"2025-02-24T21:25:11.898750Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test_continuation(model, tokenizer):# Example queries\n    queries = [\n        \"SI is used when processes are either too complex to gain insight using first principles, i.e. physical laws, or the calculation is too costly in terms of time or resources. Its goal is to\",\n        \"In pulsating or oscillating flow, heat transfer can damp, but also drive instabilities.\",\n        \"The unit impulse response is a time domain model. It shows\",\n    ]\n    \n    # Generate responses\n    for query in queries:\n            \n        inputs = tokenizer(query, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n        \n        outputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n        \n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        print(f\"\\nQuery: {query}\")\n        print(f\"\\nResponse: {text}\")\n        print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:11.900319Z","iopub.execute_input":"2025-02-24T21:25:11.900645Z","iopub.status.idle":"2025-02-24T21:25:11.905569Z","shell.execute_reply.started":"2025-02-24T21:25:11.900599Z","shell.execute_reply":"2025-02-24T21:25:11.904828Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"### 6.1 Answers to the test questions by the instruct model w/o LoRA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:11.906383Z","iopub.execute_input":"2025-02-24T21:25:11.906593Z","iopub.status.idle":"2025-02-24T21:25:11.920374Z","shell.execute_reply.started":"2025-02-24T21:25:11.906575Z","shell.execute_reply":"2025-02-24T21:25:11.919673Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### 6.2 Answers to the test questions by the adapted model","metadata":{}},{"cell_type":"code","source":"test_continuation(base_model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:11.921121Z","iopub.execute_input":"2025-02-24T21:25:11.921359Z","iopub.status.idle":"2025-02-24T21:25:40.683001Z","shell.execute_reply.started":"2025-02-24T21:25:11.921340Z","shell.execute_reply":"2025-02-24T21:25:40.682187Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nQuery: SI is used when processes are either too complex to gain insight using first principles, i.e. physical laws, or the calculation is too costly in terms of time or resources. Its goal is to\n\nResponse: SI is used when processes are either too complex to gain insight using first principles, i.e. physical laws, or the calculation is too costly in terms of time or resources. Its goal is to gain insight into the system and to predict its behavior.\n\nThe is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nQuery: In pulsating or oscillating flow, heat transfer can damp, but also drive instabilities.\n\nResponse: In pulsating or oscillating flow, heat transfer can damp, but also drive instabilities. The of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n--------------------------------------------------------------------------------\n\nQuery: The unit impulse response is a time domain model. It shows\n\nResponse: The unit impulse response is a time domain model. It shows the response of a system to a unit impulse. The unit impulse is a signal that is zero everywhere except at t=0 where it is 1. The unit impulse is a signal that is zero everywhere except at t=0 where it is 1. The unit impulse is a signal that is zero everywhere except at t= where it.\n\nThe unit impulse is a signal that is zero everywhere except at where it. The unit imp is a signal that is zero everywhere except at where it. The unit is a signal that zero everywhere except where. The unit is a signal zero everywhere where. The is a signal zero everywhere. The is signal everywhere. The is everywhere. The everywhere. The everywhere. The everywhere\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"test_continuation(peft_model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T21:25:40.683927Z","iopub.execute_input":"2025-02-24T21:25:40.684226Z","iopub.status.idle":"2025-02-24T21:26:09.098178Z","shell.execute_reply.started":"2025-02-24T21:25:40.684190Z","shell.execute_reply":"2025-02-24T21:26:09.097266Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nQuery: SI is used when processes are either too complex to gain insight using first principles, i.e. physical laws, or the calculation is too costly in terms of time or resources. Its goal is to\n\nResponse: SI is used when processes are either too complex to gain insight using first principles, i.e. physical laws, or the calculation is too costly in terms of time or resources. Its goal is to gain insight into the system and to predict its behavior.\n\nThe is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that describe the system. is a set of equations that\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nQuery: In pulsating or oscillating flow, heat transfer can damp, but also drive instabilities.\n\nResponse: In pulsating or oscillating flow, heat transfer can damp, but also drive instabilities. The of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of of\n--------------------------------------------------------------------------------\n\nQuery: The unit impulse response is a time domain model. It shows\n\nResponse: The unit impulse response is a time domain model. It shows the response of a system to a unit impulse. The unit impulse is a signal that is zero everywhere except at t=0 where it is 1. The unit impulse is a signal that is zero everywhere except at t=0 where it is 1. The unit impulse is a signal that is zero everywhere except at t= where it.\n\nThe unit impulse is a signal that is zero everywhere except at where it. The unit imp is a signal that is zero everywhere except at where it. The unit is a signal that zero everywhere except where. The unit is a signal zero everywhere where. The is a signal zero everywhere. The is signal everywhere. The is everywhere. The everywhere. The everywhere. The everywhere\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":18}]}