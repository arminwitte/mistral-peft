{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation using QLoRA\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract sentences from a technical PDF\n",
    "2. Prepare MLM training data\n",
    "3. Fine-tune a language model using QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SentenceExtractor, MLMPreprocessor, load_base_model, prepare_for_training, generate_response\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Sentences from PDF\n",
    "\n",
    "First, we'll extract and clean sentences from the dissertation PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize extractor\n",
    "# extractor = SentenceExtractor(language=\"en\")  # or \"de\" for German\n",
    "\n",
    "# # Process the PDF\n",
    "# pdf_path = \"Dissertation.pdf\"\n",
    "# extraction_result = extractor.process_document(\n",
    "#     pdf_path,\n",
    "#     output_path=\"extracted_sentences.json\"\n",
    "# )\n",
    "\n",
    "# print(f\"Extracted {extraction_result['metadata']['num_sentences']} sentences\")\n",
    "\n",
    "# # Preview some sentences\n",
    "# print(\"\\nExample sentences:\")\n",
    "# for sentence in extraction_result['sentences'][:3]:\n",
    "#     print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare MLM Training Data\n",
    "\n",
    "Now we'll create masked language modeling examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\"  # Or the specific quantized version if you are using one.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MLM examples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10340/10340 [00:00<00:00, 16349.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee83497cf9c4289938906b7b090ef04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dbedb2acf74174a8e161a720adee5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9306 training and 1034 validation examples\n",
      "\n",
      "Example input:\n",
      "<s><unk>ische Universit√§t M√ºnchen Institut f√ºr<unk>ner<unk>iete<unk>ik Professur f√ºr Therm<unk>luiddynamik Dynamics<unk><unk>steady Heat Transfer and Skin Friction in Pulsating<unk> Across<unk> C<unk>inder Armin Witte Vollst√§ndiger Abdruck der von der Fakult√§t f√ºr<unk>chinenwesen der Technischen Universit√§t M√ºnchen zur Erlangung des akademischen Grades eines<unk><unk><unk>OR ‚Äì IN<unk>IEURS genehm<unk> Dissertation.\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = MLMPreprocessor(\n",
    "    tokenizer_name=\"mistralai/Mistral-7B-v0.3\",\n",
    "    mask_probability=0.15,\n",
    "    max_length=512,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "datasets = preprocessor.process_files(\n",
    "    [\"extracted_sentences.json\"],\n",
    "    output_dir=\"processed_data\",\n",
    "    train_split=0.9\n",
    ")\n",
    "\n",
    "print(f\"Created {len(datasets['train'])} training and {len(datasets['val'])} validation examples\")\n",
    "\n",
    "# Preview a training example\n",
    "example = datasets['train'][0]\n",
    "print(\"\\nExample input:\")\n",
    "print(preprocessor.tokenizer.decode(example['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 4519,\n",
       " 25058,\n",
       " 25130,\n",
       " 21697,\n",
       " 4214,\n",
       " 0,\n",
       " 1847,\n",
       " 0,\n",
       " 2064,\n",
       " 29474,\n",
       " 0,\n",
       " 1617,\n",
       " 8797,\n",
       " 1092,\n",
       " 4214,\n",
       " 1310,\n",
       " 1626,\n",
       " 0,\n",
       " 9840,\n",
       " 3326,\n",
       " 4560,\n",
       " 1617,\n",
       " 1152,\n",
       " 26473,\n",
       " 0,\n",
       " 0,\n",
       " 29045,\n",
       " 29492,\n",
       " 24959,\n",
       " 25737,\n",
       " 1072,\n",
       " 4659,\n",
       " 1030,\n",
       " 2129,\n",
       " 3801,\n",
       " 1065,\n",
       " 1135,\n",
       " 8318,\n",
       " 1845,\n",
       " 0,\n",
       " 5636,\n",
       " 2324,\n",
       " 0,\n",
       " 1102,\n",
       " 0,\n",
       " 5820,\n",
       " 1778,\n",
       " 2008,\n",
       " 1162,\n",
       " 26823,\n",
       " 1318,\n",
       " 1561,\n",
       " 22359,\n",
       " 5654,\n",
       " 19154,\n",
       " 1319,\n",
       " 1374,\n",
       " 1659,\n",
       " 2576,\n",
       " 1659,\n",
       " 1169,\n",
       " 1259,\n",
       " 1285,\n",
       " 6059,\n",
       " 4214,\n",
       " 0,\n",
       " 1106,\n",
       " 10052,\n",
       " 29495,\n",
       " 24436,\n",
       " 1659,\n",
       " 7540,\n",
       " 3864,\n",
       " 25058,\n",
       " 25130,\n",
       " 6924,\n",
       " 3620,\n",
       " 5498,\n",
       " 1737,\n",
       " 1402,\n",
       " 13835,\n",
       " 5015,\n",
       " 3864,\n",
       " 2546,\n",
       " 3318,\n",
       " 13299,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1785,\n",
       " 1532,\n",
       " 3461,\n",
       " 0,\n",
       " 8221,\n",
       " 2758,\n",
       " 29503,\n",
       " 17966,\n",
       " 17681,\n",
       " 0,\n",
       " 4201,\n",
       " 1889,\n",
       " 1120,\n",
       " 29491]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Model\n",
    "\n",
    "We'll now load the base model and prepare it for QLoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01487281869f403ea04ac1cb5e715f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\letsg\\git\\mistral-peft\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\letsg\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa76af5a8b741f99e555d26d31d21fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaf92a2c45b4f27b9c88b20bcbac13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5537788b9ca4acb8bf71a2ab9c51aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7bcb08cf354f3dbe1b57eff15a291e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff9ba54c04f4c28a70e8b26b962f0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d12fed3b8a4f48a073028dd993e13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "c:\\Users\\letsg\\git\\mistral-peft\\venv\\Lib\\site-packages\\bitsandbytes\\backends\\cpu_xpu_common.py:29: UserWarning: g++ not found, torch.compile disabled for CPU/XPU.\n",
      "  warnings.warn(\"g++ not found, torch.compile disabled for CPU/XPU.\")\n"
     ]
    }
   ],
   "source": [
    "# Load base model and tokenizer\n",
    "model, tokenizer = load_base_model()\n",
    "\n",
    "# Prepare for LoRA training\n",
    "model = prepare_for_training(\n",
    "    model,\n",
    "    lora_r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Now we'll fine-tune the model on our domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\letsg\\git\\mistral-peft\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets['train'],\n",
    "    eval_dataset=datasets['val']\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Model\n",
    "\n",
    "Let's test the fine-tuned model with some domain-specific queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries about heat transfer and fluid dynamics\n",
    "queries = [\n",
    "    \"Explain the relationship between pulsating crossflow and heat transfer efficiency.\",\n",
    "    \"What are the key factors affecting skin friction in the experimental setup?\",\n",
    "    \"Summarize the main findings regarding heat transfer dynamics in the study.\",\n",
    "    \"How does the Reynolds number influence the observed phenomena?\"\n",
    "]\n",
    "\n",
    "# Generate responses\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    response = generate_response(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        query,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model\n",
    "\n",
    "Finally, let's save our fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = Path(\"./final_model\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
