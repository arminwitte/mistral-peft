{
  "best_metric": 3.130455255508423,
  "best_model_checkpoint": "./results/checkpoint-231",
  "epoch": 2.969827586206897,
  "eval_steps": 500,
  "global_step": 231,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01293103448275862,
      "grad_norm": NaN,
      "learning_rate": 0.0003,
      "loss": 35.1608,
      "step": 1
    },
    {
      "epoch": 0.02586206896551724,
      "grad_norm": 9.498631477355957,
      "learning_rate": 0.0002987012987012987,
      "loss": 31.9126,
      "step": 2
    },
    {
      "epoch": 0.03879310344827586,
      "grad_norm": Infinity,
      "learning_rate": 0.0002987012987012987,
      "loss": 30.7244,
      "step": 3
    },
    {
      "epoch": 0.05172413793103448,
      "grad_norm": 10.979710578918457,
      "learning_rate": 0.0002974025974025974,
      "loss": 29.3,
      "step": 4
    },
    {
      "epoch": 0.06465517241379311,
      "grad_norm": 20.201818466186523,
      "learning_rate": 0.0002961038961038961,
      "loss": 25.5387,
      "step": 5
    },
    {
      "epoch": 0.07758620689655173,
      "grad_norm": Infinity,
      "learning_rate": 0.0002961038961038961,
      "loss": 26.5997,
      "step": 6
    },
    {
      "epoch": 0.09051724137931035,
      "grad_norm": Infinity,
      "learning_rate": 0.0002961038961038961,
      "loss": 30.392,
      "step": 7
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 40.31182861328125,
      "learning_rate": 0.0002948051948051948,
      "loss": 27.9939,
      "step": 8
    },
    {
      "epoch": 0.11637931034482758,
      "grad_norm": 26.062610626220703,
      "learning_rate": 0.00029350649350649345,
      "loss": 21.8951,
      "step": 9
    },
    {
      "epoch": 0.12931034482758622,
      "grad_norm": 20.186119079589844,
      "learning_rate": 0.00029220779220779215,
      "loss": 19.3927,
      "step": 10
    },
    {
      "epoch": 0.14224137931034483,
      "grad_norm": 12.654959678649902,
      "learning_rate": 0.0002909090909090909,
      "loss": 17.9054,
      "step": 11
    },
    {
      "epoch": 0.15517241379310345,
      "grad_norm": 24.66238784790039,
      "learning_rate": 0.00028961038961038956,
      "loss": 17.7884,
      "step": 12
    },
    {
      "epoch": 0.16810344827586207,
      "grad_norm": 14.557443618774414,
      "learning_rate": 0.00028831168831168827,
      "loss": 17.2697,
      "step": 13
    },
    {
      "epoch": 0.1810344827586207,
      "grad_norm": 44.95820236206055,
      "learning_rate": 0.000287012987012987,
      "loss": 13.6747,
      "step": 14
    },
    {
      "epoch": 0.1939655172413793,
      "grad_norm": 14.186003684997559,
      "learning_rate": 0.0002857142857142857,
      "loss": 16.066,
      "step": 15
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 8.028042793273926,
      "learning_rate": 0.0002844155844155844,
      "loss": 12.5398,
      "step": 16
    },
    {
      "epoch": 0.21982758620689655,
      "grad_norm": 6.74392032623291,
      "learning_rate": 0.0002831168831168831,
      "loss": 15.262,
      "step": 17
    },
    {
      "epoch": 0.23275862068965517,
      "grad_norm": 6.337515354156494,
      "learning_rate": 0.0002818181818181818,
      "loss": 15.2952,
      "step": 18
    },
    {
      "epoch": 0.24568965517241378,
      "grad_norm": 6.145899295806885,
      "learning_rate": 0.0002805194805194805,
      "loss": 14.9373,
      "step": 19
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 8.621206283569336,
      "learning_rate": 0.0002792207792207792,
      "loss": 14.8593,
      "step": 20
    },
    {
      "epoch": 0.27155172413793105,
      "grad_norm": 5.0739827156066895,
      "learning_rate": 0.0002779220779220779,
      "loss": 14.6853,
      "step": 21
    },
    {
      "epoch": 0.28448275862068967,
      "grad_norm": 3.915724754333496,
      "learning_rate": 0.00027662337662337657,
      "loss": 14.5162,
      "step": 22
    },
    {
      "epoch": 0.2974137931034483,
      "grad_norm": 4.684821605682373,
      "learning_rate": 0.0002753246753246753,
      "loss": 14.7288,
      "step": 23
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 4.584661960601807,
      "learning_rate": 0.00027402597402597403,
      "loss": 13.4752,
      "step": 24
    },
    {
      "epoch": 0.3232758620689655,
      "grad_norm": 4.488706588745117,
      "learning_rate": 0.0002727272727272727,
      "loss": 14.558,
      "step": 25
    },
    {
      "epoch": 0.33620689655172414,
      "grad_norm": 3.4738681316375732,
      "learning_rate": 0.0002714285714285714,
      "loss": 13.0702,
      "step": 26
    },
    {
      "epoch": 0.34913793103448276,
      "grad_norm": 4.489497184753418,
      "learning_rate": 0.0002701298701298701,
      "loss": 13.2133,
      "step": 27
    },
    {
      "epoch": 0.3620689655172414,
      "grad_norm": 3.348630428314209,
      "learning_rate": 0.0002688311688311688,
      "loss": 13.708,
      "step": 28
    },
    {
      "epoch": 0.375,
      "grad_norm": 3.9648547172546387,
      "learning_rate": 0.0002675324675324675,
      "loss": 12.8631,
      "step": 29
    },
    {
      "epoch": 0.3879310344827586,
      "grad_norm": 4.0243120193481445,
      "learning_rate": 0.0002662337662337662,
      "loss": 12.6698,
      "step": 30
    },
    {
      "epoch": 0.40086206896551724,
      "grad_norm": 2.8896636962890625,
      "learning_rate": 0.0002649350649350649,
      "loss": 12.3001,
      "step": 31
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 3.873629570007324,
      "learning_rate": 0.0002636363636363636,
      "loss": 12.6123,
      "step": 32
    },
    {
      "epoch": 0.4267241379310345,
      "grad_norm": 5.52213716506958,
      "learning_rate": 0.00026233766233766233,
      "loss": 13.0068,
      "step": 33
    },
    {
      "epoch": 0.4396551724137931,
      "grad_norm": 3.8918497562408447,
      "learning_rate": 0.000261038961038961,
      "loss": 12.7296,
      "step": 34
    },
    {
      "epoch": 0.4525862068965517,
      "grad_norm": 4.151779651641846,
      "learning_rate": 0.00025974025974025974,
      "loss": 12.7894,
      "step": 35
    },
    {
      "epoch": 0.46551724137931033,
      "grad_norm": 4.28049898147583,
      "learning_rate": 0.00025844155844155845,
      "loss": 12.176,
      "step": 36
    },
    {
      "epoch": 0.47844827586206895,
      "grad_norm": 3.344611883163452,
      "learning_rate": 0.0002571428571428571,
      "loss": 9.3734,
      "step": 37
    },
    {
      "epoch": 0.49137931034482757,
      "grad_norm": 4.029280185699463,
      "learning_rate": 0.0002558441558441558,
      "loss": 11.7098,
      "step": 38
    },
    {
      "epoch": 0.5043103448275862,
      "grad_norm": 4.853503227233887,
      "learning_rate": 0.0002545454545454545,
      "loss": 12.6607,
      "step": 39
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 3.959879159927368,
      "learning_rate": 0.0002532467532467532,
      "loss": 11.6397,
      "step": 40
    },
    {
      "epoch": 0.5301724137931034,
      "grad_norm": 4.6632795333862305,
      "learning_rate": 0.0002519480519480519,
      "loss": 12.8616,
      "step": 41
    },
    {
      "epoch": 0.5431034482758621,
      "grad_norm": 4.517310619354248,
      "learning_rate": 0.0002506493506493506,
      "loss": 13.1835,
      "step": 42
    },
    {
      "epoch": 0.5560344827586207,
      "grad_norm": 3.2161476612091064,
      "learning_rate": 0.00024935064935064933,
      "loss": 12.4197,
      "step": 43
    },
    {
      "epoch": 0.5689655172413793,
      "grad_norm": 3.9185047149658203,
      "learning_rate": 0.00024805194805194804,
      "loss": 11.832,
      "step": 44
    },
    {
      "epoch": 0.5818965517241379,
      "grad_norm": 4.3378987312316895,
      "learning_rate": 0.00024675324675324674,
      "loss": 12.0122,
      "step": 45
    },
    {
      "epoch": 0.5948275862068966,
      "grad_norm": 4.143579006195068,
      "learning_rate": 0.00024545454545454545,
      "loss": 12.1129,
      "step": 46
    },
    {
      "epoch": 0.6077586206896551,
      "grad_norm": 3.166975736618042,
      "learning_rate": 0.00024415584415584415,
      "loss": 12.8142,
      "step": 47
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 3.494629383087158,
      "learning_rate": 0.00024285714285714283,
      "loss": 10.8532,
      "step": 48
    },
    {
      "epoch": 0.6336206896551724,
      "grad_norm": 3.337921380996704,
      "learning_rate": 0.00024155844155844154,
      "loss": 11.8737,
      "step": 49
    },
    {
      "epoch": 0.646551724137931,
      "grad_norm": 3.7817816734313965,
      "learning_rate": 0.00024025974025974024,
      "loss": 11.0386,
      "step": 50
    },
    {
      "epoch": 0.6594827586206896,
      "grad_norm": 3.4227616786956787,
      "learning_rate": 0.00023896103896103895,
      "loss": 12.0516,
      "step": 51
    },
    {
      "epoch": 0.6724137931034483,
      "grad_norm": 3.824897050857544,
      "learning_rate": 0.00023766233766233765,
      "loss": 10.0321,
      "step": 52
    },
    {
      "epoch": 0.6853448275862069,
      "grad_norm": 4.330126762390137,
      "learning_rate": 0.00023636363636363633,
      "loss": 12.1112,
      "step": 53
    },
    {
      "epoch": 0.6982758620689655,
      "grad_norm": 2.9986732006073,
      "learning_rate": 0.00023506493506493504,
      "loss": 11.4283,
      "step": 54
    },
    {
      "epoch": 0.7112068965517241,
      "grad_norm": 3.0932910442352295,
      "learning_rate": 0.00023376623376623374,
      "loss": 10.9379,
      "step": 55
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 3.3554084300994873,
      "learning_rate": 0.00023246753246753242,
      "loss": 10.0465,
      "step": 56
    },
    {
      "epoch": 0.7370689655172413,
      "grad_norm": 3.003232479095459,
      "learning_rate": 0.00023116883116883116,
      "loss": 10.3131,
      "step": 57
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.816528081893921,
      "learning_rate": 0.00022987012987012986,
      "loss": 8.3072,
      "step": 58
    },
    {
      "epoch": 0.7629310344827587,
      "grad_norm": 3.3908424377441406,
      "learning_rate": 0.00022857142857142854,
      "loss": 11.476,
      "step": 59
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 3.0751357078552246,
      "learning_rate": 0.00022727272727272725,
      "loss": 9.5416,
      "step": 60
    },
    {
      "epoch": 0.7887931034482759,
      "grad_norm": 4.440732955932617,
      "learning_rate": 0.00022597402597402595,
      "loss": 10.3947,
      "step": 61
    },
    {
      "epoch": 0.8017241379310345,
      "grad_norm": 3.1682794094085693,
      "learning_rate": 0.00022467532467532463,
      "loss": 10.8112,
      "step": 62
    },
    {
      "epoch": 0.8146551724137931,
      "grad_norm": 4.872435092926025,
      "learning_rate": 0.00022337662337662336,
      "loss": 9.4813,
      "step": 63
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 4.583402633666992,
      "learning_rate": 0.00022207792207792207,
      "loss": 10.7395,
      "step": 64
    },
    {
      "epoch": 0.8405172413793104,
      "grad_norm": 2.910429000854492,
      "learning_rate": 0.00022077922077922075,
      "loss": 11.1816,
      "step": 65
    },
    {
      "epoch": 0.853448275862069,
      "grad_norm": 3.464561939239502,
      "learning_rate": 0.00021948051948051945,
      "loss": 9.2225,
      "step": 66
    },
    {
      "epoch": 0.8663793103448276,
      "grad_norm": 3.3045012950897217,
      "learning_rate": 0.00021818181818181816,
      "loss": 10.4823,
      "step": 67
    },
    {
      "epoch": 0.8793103448275862,
      "grad_norm": 3.530181407928467,
      "learning_rate": 0.00021688311688311684,
      "loss": 10.5763,
      "step": 68
    },
    {
      "epoch": 0.8922413793103449,
      "grad_norm": 3.5994646549224854,
      "learning_rate": 0.00021558441558441557,
      "loss": 10.6001,
      "step": 69
    },
    {
      "epoch": 0.9051724137931034,
      "grad_norm": 2.8888847827911377,
      "learning_rate": 0.00021428571428571427,
      "loss": 11.4698,
      "step": 70
    },
    {
      "epoch": 0.9181034482758621,
      "grad_norm": 3.3341593742370605,
      "learning_rate": 0.00021298701298701298,
      "loss": 8.9128,
      "step": 71
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 2.9974467754364014,
      "learning_rate": 0.00021168831168831166,
      "loss": 10.4517,
      "step": 72
    },
    {
      "epoch": 0.9439655172413793,
      "grad_norm": 3.493880033493042,
      "learning_rate": 0.00021038961038961036,
      "loss": 11.2871,
      "step": 73
    },
    {
      "epoch": 0.9568965517241379,
      "grad_norm": 2.766749382019043,
      "learning_rate": 0.0002090909090909091,
      "loss": 9.5786,
      "step": 74
    },
    {
      "epoch": 0.9698275862068966,
      "grad_norm": 3.636368751525879,
      "learning_rate": 0.00020779220779220778,
      "loss": 10.0376,
      "step": 75
    },
    {
      "epoch": 0.9827586206896551,
      "grad_norm": 3.9716851711273193,
      "learning_rate": 0.00020649350649350648,
      "loss": 10.6453,
      "step": 76
    },
    {
      "epoch": 0.9956896551724138,
      "grad_norm": 3.3088743686676025,
      "learning_rate": 0.0002051948051948052,
      "loss": 10.8774,
      "step": 77
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.7290042638778687,
      "learning_rate": 0.00020389610389610387,
      "loss": 3.1406,
      "step": 78
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.3748056888580322,
      "eval_runtime": 121.4753,
      "eval_samples_per_second": 0.214,
      "eval_steps_per_second": 0.214,
      "step": 78
    },
    {
      "epoch": 1.0129310344827587,
      "grad_norm": 3.90639066696167,
      "learning_rate": 0.00020259740259740257,
      "loss": 10.3371,
      "step": 79
    },
    {
      "epoch": 1.0258620689655173,
      "grad_norm": 3.4302334785461426,
      "learning_rate": 0.0002012987012987013,
      "loss": 9.7531,
      "step": 80
    },
    {
      "epoch": 1.0387931034482758,
      "grad_norm": 2.7183854579925537,
      "learning_rate": 0.00019999999999999998,
      "loss": 8.4374,
      "step": 81
    },
    {
      "epoch": 1.0517241379310345,
      "grad_norm": 3.2216830253601074,
      "learning_rate": 0.0001987012987012987,
      "loss": 10.0148,
      "step": 82
    },
    {
      "epoch": 1.0646551724137931,
      "grad_norm": 3.2525274753570557,
      "learning_rate": 0.0001974025974025974,
      "loss": 9.9788,
      "step": 83
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 3.2122344970703125,
      "learning_rate": 0.00019610389610389607,
      "loss": 10.0804,
      "step": 84
    },
    {
      "epoch": 1.0905172413793103,
      "grad_norm": 3.104163646697998,
      "learning_rate": 0.00019480519480519478,
      "loss": 10.3254,
      "step": 85
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 3.3647541999816895,
      "learning_rate": 0.0001935064935064935,
      "loss": 9.3435,
      "step": 86
    },
    {
      "epoch": 1.1163793103448276,
      "grad_norm": 7.6382904052734375,
      "learning_rate": 0.0001922077922077922,
      "loss": 10.2176,
      "step": 87
    },
    {
      "epoch": 1.1293103448275863,
      "grad_norm": 3.9379637241363525,
      "learning_rate": 0.0001909090909090909,
      "loss": 9.7862,
      "step": 88
    },
    {
      "epoch": 1.1422413793103448,
      "grad_norm": 3.733790397644043,
      "learning_rate": 0.0001896103896103896,
      "loss": 10.7702,
      "step": 89
    },
    {
      "epoch": 1.1551724137931034,
      "grad_norm": 2.862022638320923,
      "learning_rate": 0.00018831168831168828,
      "loss": 8.3862,
      "step": 90
    },
    {
      "epoch": 1.168103448275862,
      "grad_norm": 3.198615312576294,
      "learning_rate": 0.00018701298701298698,
      "loss": 10.6849,
      "step": 91
    },
    {
      "epoch": 1.1810344827586208,
      "grad_norm": 3.7742857933044434,
      "learning_rate": 0.00018571428571428572,
      "loss": 9.1489,
      "step": 92
    },
    {
      "epoch": 1.1939655172413792,
      "grad_norm": 3.539418935775757,
      "learning_rate": 0.0001844155844155844,
      "loss": 10.6529,
      "step": 93
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 3.5666630268096924,
      "learning_rate": 0.0001831168831168831,
      "loss": 10.7595,
      "step": 94
    },
    {
      "epoch": 1.2198275862068966,
      "grad_norm": 3.0167009830474854,
      "learning_rate": 0.0001818181818181818,
      "loss": 10.5005,
      "step": 95
    },
    {
      "epoch": 1.2327586206896552,
      "grad_norm": 3.0873451232910156,
      "learning_rate": 0.0001805194805194805,
      "loss": 9.7142,
      "step": 96
    },
    {
      "epoch": 1.2456896551724137,
      "grad_norm": 3.8717472553253174,
      "learning_rate": 0.0001792207792207792,
      "loss": 10.3182,
      "step": 97
    },
    {
      "epoch": 1.2586206896551724,
      "grad_norm": 3.0050768852233887,
      "learning_rate": 0.0001779220779220779,
      "loss": 9.6228,
      "step": 98
    },
    {
      "epoch": 1.271551724137931,
      "grad_norm": 3.7609457969665527,
      "learning_rate": 0.00017662337662337663,
      "loss": 10.6724,
      "step": 99
    },
    {
      "epoch": 1.2844827586206897,
      "grad_norm": 4.418886184692383,
      "learning_rate": 0.0001753246753246753,
      "loss": 10.8386,
      "step": 100
    },
    {
      "epoch": 1.2974137931034484,
      "grad_norm": 3.475203514099121,
      "learning_rate": 0.00017402597402597401,
      "loss": 9.7702,
      "step": 101
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 3.54891037940979,
      "learning_rate": 0.00017272727272727272,
      "loss": 10.902,
      "step": 102
    },
    {
      "epoch": 1.3232758620689655,
      "grad_norm": 3.6132125854492188,
      "learning_rate": 0.0001714285714285714,
      "loss": 7.685,
      "step": 103
    },
    {
      "epoch": 1.3362068965517242,
      "grad_norm": 3.5246353149414062,
      "learning_rate": 0.0001701298701298701,
      "loss": 10.0989,
      "step": 104
    },
    {
      "epoch": 1.3491379310344827,
      "grad_norm": 3.6160497665405273,
      "learning_rate": 0.00016883116883116884,
      "loss": 10.5049,
      "step": 105
    },
    {
      "epoch": 1.3620689655172413,
      "grad_norm": 3.327263593673706,
      "learning_rate": 0.00016753246753246751,
      "loss": 10.0084,
      "step": 106
    },
    {
      "epoch": 1.375,
      "grad_norm": 3.201719284057617,
      "learning_rate": 0.00016623376623376622,
      "loss": 10.3016,
      "step": 107
    },
    {
      "epoch": 1.3879310344827587,
      "grad_norm": 6.096494197845459,
      "learning_rate": 0.00016493506493506493,
      "loss": 10.0649,
      "step": 108
    },
    {
      "epoch": 1.4008620689655173,
      "grad_norm": 3.4147119522094727,
      "learning_rate": 0.0001636363636363636,
      "loss": 8.9458,
      "step": 109
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 3.808351755142212,
      "learning_rate": 0.0001623376623376623,
      "loss": 10.1294,
      "step": 110
    },
    {
      "epoch": 1.4267241379310345,
      "grad_norm": 3.2952849864959717,
      "learning_rate": 0.00016103896103896104,
      "loss": 10.9406,
      "step": 111
    },
    {
      "epoch": 1.4396551724137931,
      "grad_norm": 3.35164475440979,
      "learning_rate": 0.00015974025974025972,
      "loss": 9.6685,
      "step": 112
    },
    {
      "epoch": 1.4525862068965516,
      "grad_norm": 3.716529130935669,
      "learning_rate": 0.00015844155844155843,
      "loss": 9.6007,
      "step": 113
    },
    {
      "epoch": 1.4655172413793103,
      "grad_norm": 3.1733248233795166,
      "learning_rate": 0.00015714285714285713,
      "loss": 9.712,
      "step": 114
    },
    {
      "epoch": 1.478448275862069,
      "grad_norm": 3.396585702896118,
      "learning_rate": 0.0001558441558441558,
      "loss": 10.8524,
      "step": 115
    },
    {
      "epoch": 1.4913793103448276,
      "grad_norm": 3.0955801010131836,
      "learning_rate": 0.00015454545454545452,
      "loss": 8.1923,
      "step": 116
    },
    {
      "epoch": 1.5043103448275863,
      "grad_norm": 2.832077741622925,
      "learning_rate": 0.00015324675324675325,
      "loss": 10.0065,
      "step": 117
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 4.427791118621826,
      "learning_rate": 0.00015194805194805193,
      "loss": 10.303,
      "step": 118
    },
    {
      "epoch": 1.5301724137931034,
      "grad_norm": 4.510751724243164,
      "learning_rate": 0.00015064935064935063,
      "loss": 8.8813,
      "step": 119
    },
    {
      "epoch": 1.543103448275862,
      "grad_norm": 3.508016347885132,
      "learning_rate": 0.00014935064935064934,
      "loss": 10.243,
      "step": 120
    },
    {
      "epoch": 1.5560344827586206,
      "grad_norm": 3.3763723373413086,
      "learning_rate": 0.00014805194805194805,
      "loss": 9.3128,
      "step": 121
    },
    {
      "epoch": 1.5689655172413794,
      "grad_norm": 3.0946271419525146,
      "learning_rate": 0.00014675324675324672,
      "loss": 9.9107,
      "step": 122
    },
    {
      "epoch": 1.581896551724138,
      "grad_norm": 3.0196030139923096,
      "learning_rate": 0.00014545454545454546,
      "loss": 9.0618,
      "step": 123
    },
    {
      "epoch": 1.5948275862068966,
      "grad_norm": 4.014636039733887,
      "learning_rate": 0.00014415584415584414,
      "loss": 10.994,
      "step": 124
    },
    {
      "epoch": 1.6077586206896552,
      "grad_norm": 3.2915942668914795,
      "learning_rate": 0.00014285714285714284,
      "loss": 9.3206,
      "step": 125
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 3.5697362422943115,
      "learning_rate": 0.00014155844155844155,
      "loss": 9.9851,
      "step": 126
    },
    {
      "epoch": 1.6336206896551724,
      "grad_norm": 3.216257333755493,
      "learning_rate": 0.00014025974025974025,
      "loss": 9.4353,
      "step": 127
    },
    {
      "epoch": 1.646551724137931,
      "grad_norm": 3.3360679149627686,
      "learning_rate": 0.00013896103896103896,
      "loss": 9.8191,
      "step": 128
    },
    {
      "epoch": 1.6594827586206895,
      "grad_norm": 3.432391881942749,
      "learning_rate": 0.00013766233766233766,
      "loss": 10.2968,
      "step": 129
    },
    {
      "epoch": 1.6724137931034484,
      "grad_norm": 3.3247623443603516,
      "learning_rate": 0.00013636363636363634,
      "loss": 9.5164,
      "step": 130
    },
    {
      "epoch": 1.6853448275862069,
      "grad_norm": 3.3683533668518066,
      "learning_rate": 0.00013506493506493505,
      "loss": 9.4234,
      "step": 131
    },
    {
      "epoch": 1.6982758620689655,
      "grad_norm": 3.5049667358398438,
      "learning_rate": 0.00013376623376623375,
      "loss": 10.5332,
      "step": 132
    },
    {
      "epoch": 1.7112068965517242,
      "grad_norm": 3.3907434940338135,
      "learning_rate": 0.00013246753246753246,
      "loss": 10.0097,
      "step": 133
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 3.4843459129333496,
      "learning_rate": 0.00013116883116883116,
      "loss": 10.5646,
      "step": 134
    },
    {
      "epoch": 1.7370689655172413,
      "grad_norm": 2.977480888366699,
      "learning_rate": 0.00012987012987012987,
      "loss": 9.2909,
      "step": 135
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.1326963901519775,
      "learning_rate": 0.00012857142857142855,
      "loss": 10.0172,
      "step": 136
    },
    {
      "epoch": 1.7629310344827587,
      "grad_norm": 3.6457619667053223,
      "learning_rate": 0.00012727272727272725,
      "loss": 9.4737,
      "step": 137
    },
    {
      "epoch": 1.7758620689655173,
      "grad_norm": 4.171846389770508,
      "learning_rate": 0.00012597402597402596,
      "loss": 10.604,
      "step": 138
    },
    {
      "epoch": 1.7887931034482758,
      "grad_norm": 4.250638008117676,
      "learning_rate": 0.00012467532467532467,
      "loss": 10.2323,
      "step": 139
    },
    {
      "epoch": 1.8017241379310345,
      "grad_norm": 3.429205894470215,
      "learning_rate": 0.00012337662337662337,
      "loss": 9.9769,
      "step": 140
    },
    {
      "epoch": 1.8146551724137931,
      "grad_norm": 3.4709386825561523,
      "learning_rate": 0.00012207792207792208,
      "loss": 10.3017,
      "step": 141
    },
    {
      "epoch": 1.8275862068965516,
      "grad_norm": 3.180453062057495,
      "learning_rate": 0.00012077922077922077,
      "loss": 7.1831,
      "step": 142
    },
    {
      "epoch": 1.8405172413793105,
      "grad_norm": 3.580900192260742,
      "learning_rate": 0.00011948051948051947,
      "loss": 8.6476,
      "step": 143
    },
    {
      "epoch": 1.853448275862069,
      "grad_norm": 3.4623281955718994,
      "learning_rate": 0.00011818181818181817,
      "loss": 10.3495,
      "step": 144
    },
    {
      "epoch": 1.8663793103448276,
      "grad_norm": 3.208652973175049,
      "learning_rate": 0.00011688311688311687,
      "loss": 8.3731,
      "step": 145
    },
    {
      "epoch": 1.8793103448275863,
      "grad_norm": 3.183626890182495,
      "learning_rate": 0.00011558441558441558,
      "loss": 8.8654,
      "step": 146
    },
    {
      "epoch": 1.8922413793103448,
      "grad_norm": 3.653122663497925,
      "learning_rate": 0.00011428571428571427,
      "loss": 9.1719,
      "step": 147
    },
    {
      "epoch": 1.9051724137931034,
      "grad_norm": 3.40657639503479,
      "learning_rate": 0.00011298701298701298,
      "loss": 10.3183,
      "step": 148
    },
    {
      "epoch": 1.918103448275862,
      "grad_norm": 3.131134510040283,
      "learning_rate": 0.00011168831168831168,
      "loss": 10.4519,
      "step": 149
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 3.2778353691101074,
      "learning_rate": 0.00011038961038961037,
      "loss": 9.204,
      "step": 150
    },
    {
      "epoch": 1.9439655172413794,
      "grad_norm": 3.291884660720825,
      "learning_rate": 0.00010909090909090908,
      "loss": 9.1019,
      "step": 151
    },
    {
      "epoch": 1.956896551724138,
      "grad_norm": 2.986346960067749,
      "learning_rate": 0.00010779220779220778,
      "loss": 7.6359,
      "step": 152
    },
    {
      "epoch": 1.9698275862068966,
      "grad_norm": 2.9058985710144043,
      "learning_rate": 0.00010649350649350649,
      "loss": 7.1165,
      "step": 153
    },
    {
      "epoch": 1.9827586206896552,
      "grad_norm": 4.196706295013428,
      "learning_rate": 0.00010519480519480518,
      "loss": 9.2609,
      "step": 154
    },
    {
      "epoch": 1.9956896551724137,
      "grad_norm": 3.339599847793579,
      "learning_rate": 0.00010389610389610389,
      "loss": 8.6662,
      "step": 155
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.755329966545105,
      "learning_rate": 0.0001025974025974026,
      "loss": 3.4999,
      "step": 156
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.174562931060791,
      "eval_runtime": 121.4693,
      "eval_samples_per_second": 0.214,
      "eval_steps_per_second": 0.214,
      "step": 156
    },
    {
      "epoch": 2.0129310344827585,
      "grad_norm": 3.2672722339630127,
      "learning_rate": 0.00010129870129870129,
      "loss": 8.8501,
      "step": 157
    },
    {
      "epoch": 2.0258620689655173,
      "grad_norm": 3.143540143966675,
      "learning_rate": 9.999999999999999e-05,
      "loss": 8.8845,
      "step": 158
    },
    {
      "epoch": 2.038793103448276,
      "grad_norm": 3.1668591499328613,
      "learning_rate": 9.87012987012987e-05,
      "loss": 9.2839,
      "step": 159
    },
    {
      "epoch": 2.0517241379310347,
      "grad_norm": 3.014901876449585,
      "learning_rate": 9.740259740259739e-05,
      "loss": 8.7996,
      "step": 160
    },
    {
      "epoch": 2.064655172413793,
      "grad_norm": 3.4079887866973877,
      "learning_rate": 9.61038961038961e-05,
      "loss": 9.7015,
      "step": 161
    },
    {
      "epoch": 2.0775862068965516,
      "grad_norm": 3.278817653656006,
      "learning_rate": 9.48051948051948e-05,
      "loss": 7.6709,
      "step": 162
    },
    {
      "epoch": 2.0905172413793105,
      "grad_norm": 2.9664313793182373,
      "learning_rate": 9.350649350649349e-05,
      "loss": 9.0613,
      "step": 163
    },
    {
      "epoch": 2.103448275862069,
      "grad_norm": 2.7666401863098145,
      "learning_rate": 9.22077922077922e-05,
      "loss": 7.4864,
      "step": 164
    },
    {
      "epoch": 2.1163793103448274,
      "grad_norm": 3.371715784072876,
      "learning_rate": 9.09090909090909e-05,
      "loss": 8.9039,
      "step": 165
    },
    {
      "epoch": 2.1293103448275863,
      "grad_norm": 2.852365016937256,
      "learning_rate": 8.96103896103896e-05,
      "loss": 7.6774,
      "step": 166
    },
    {
      "epoch": 2.1422413793103448,
      "grad_norm": 3.760552167892456,
      "learning_rate": 8.831168831168831e-05,
      "loss": 10.0421,
      "step": 167
    },
    {
      "epoch": 2.1551724137931036,
      "grad_norm": 3.524148941040039,
      "learning_rate": 8.701298701298701e-05,
      "loss": 9.1935,
      "step": 168
    },
    {
      "epoch": 2.168103448275862,
      "grad_norm": 2.967514991760254,
      "learning_rate": 8.57142857142857e-05,
      "loss": 9.4984,
      "step": 169
    },
    {
      "epoch": 2.1810344827586206,
      "grad_norm": 2.799694061279297,
      "learning_rate": 8.441558441558442e-05,
      "loss": 8.5484,
      "step": 170
    },
    {
      "epoch": 2.1939655172413794,
      "grad_norm": 3.231218099594116,
      "learning_rate": 8.311688311688311e-05,
      "loss": 8.7604,
      "step": 171
    },
    {
      "epoch": 2.206896551724138,
      "grad_norm": 4.446913719177246,
      "learning_rate": 8.18181818181818e-05,
      "loss": 9.048,
      "step": 172
    },
    {
      "epoch": 2.2198275862068964,
      "grad_norm": 3.2121424674987793,
      "learning_rate": 8.051948051948052e-05,
      "loss": 8.8338,
      "step": 173
    },
    {
      "epoch": 2.2327586206896552,
      "grad_norm": 3.405587911605835,
      "learning_rate": 7.922077922077921e-05,
      "loss": 9.313,
      "step": 174
    },
    {
      "epoch": 2.2456896551724137,
      "grad_norm": 3.348043441772461,
      "learning_rate": 7.79220779220779e-05,
      "loss": 9.1429,
      "step": 175
    },
    {
      "epoch": 2.2586206896551726,
      "grad_norm": 3.4277656078338623,
      "learning_rate": 7.662337662337662e-05,
      "loss": 8.5311,
      "step": 176
    },
    {
      "epoch": 2.271551724137931,
      "grad_norm": 3.3681347370147705,
      "learning_rate": 7.532467532467532e-05,
      "loss": 9.0663,
      "step": 177
    },
    {
      "epoch": 2.2844827586206895,
      "grad_norm": 3.1952853202819824,
      "learning_rate": 7.402597402597402e-05,
      "loss": 9.2917,
      "step": 178
    },
    {
      "epoch": 2.2974137931034484,
      "grad_norm": 3.533167839050293,
      "learning_rate": 7.272727272727273e-05,
      "loss": 9.7377,
      "step": 179
    },
    {
      "epoch": 2.310344827586207,
      "grad_norm": 3.6378791332244873,
      "learning_rate": 7.142857142857142e-05,
      "loss": 9.2059,
      "step": 180
    },
    {
      "epoch": 2.3232758620689653,
      "grad_norm": 3.8455283641815186,
      "learning_rate": 7.012987012987013e-05,
      "loss": 10.5721,
      "step": 181
    },
    {
      "epoch": 2.336206896551724,
      "grad_norm": 3.3025989532470703,
      "learning_rate": 6.883116883116883e-05,
      "loss": 9.7349,
      "step": 182
    },
    {
      "epoch": 2.3491379310344827,
      "grad_norm": 3.8044307231903076,
      "learning_rate": 6.753246753246752e-05,
      "loss": 7.8736,
      "step": 183
    },
    {
      "epoch": 2.3620689655172415,
      "grad_norm": 2.920682668685913,
      "learning_rate": 6.623376623376623e-05,
      "loss": 6.751,
      "step": 184
    },
    {
      "epoch": 2.375,
      "grad_norm": 3.699033498764038,
      "learning_rate": 6.493506493506494e-05,
      "loss": 9.3395,
      "step": 185
    },
    {
      "epoch": 2.3879310344827585,
      "grad_norm": 3.566956043243408,
      "learning_rate": 6.363636363636363e-05,
      "loss": 10.5907,
      "step": 186
    },
    {
      "epoch": 2.4008620689655173,
      "grad_norm": 3.575056314468384,
      "learning_rate": 6.233766233766233e-05,
      "loss": 10.5777,
      "step": 187
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 3.305751323699951,
      "learning_rate": 6.103896103896104e-05,
      "loss": 9.531,
      "step": 188
    },
    {
      "epoch": 2.4267241379310347,
      "grad_norm": 3.348289728164673,
      "learning_rate": 5.974025974025974e-05,
      "loss": 9.4593,
      "step": 189
    },
    {
      "epoch": 2.439655172413793,
      "grad_norm": 3.2747678756713867,
      "learning_rate": 5.8441558441558436e-05,
      "loss": 9.1435,
      "step": 190
    },
    {
      "epoch": 2.4525862068965516,
      "grad_norm": 3.9728009700775146,
      "learning_rate": 5.7142857142857135e-05,
      "loss": 10.125,
      "step": 191
    },
    {
      "epoch": 2.4655172413793105,
      "grad_norm": 3.0724027156829834,
      "learning_rate": 5.584415584415584e-05,
      "loss": 7.8329,
      "step": 192
    },
    {
      "epoch": 2.478448275862069,
      "grad_norm": 3.4657135009765625,
      "learning_rate": 5.454545454545454e-05,
      "loss": 9.442,
      "step": 193
    },
    {
      "epoch": 2.4913793103448274,
      "grad_norm": 3.5811727046966553,
      "learning_rate": 5.3246753246753245e-05,
      "loss": 9.4683,
      "step": 194
    },
    {
      "epoch": 2.5043103448275863,
      "grad_norm": 3.5565237998962402,
      "learning_rate": 5.1948051948051944e-05,
      "loss": 8.8624,
      "step": 195
    },
    {
      "epoch": 2.5172413793103448,
      "grad_norm": 3.6745493412017822,
      "learning_rate": 5.064935064935064e-05,
      "loss": 9.8458,
      "step": 196
    },
    {
      "epoch": 2.530172413793103,
      "grad_norm": 3.3290581703186035,
      "learning_rate": 4.935064935064935e-05,
      "loss": 9.3436,
      "step": 197
    },
    {
      "epoch": 2.543103448275862,
      "grad_norm": 3.214994192123413,
      "learning_rate": 4.805194805194805e-05,
      "loss": 9.4974,
      "step": 198
    },
    {
      "epoch": 2.5560344827586206,
      "grad_norm": 3.193291187286377,
      "learning_rate": 4.6753246753246746e-05,
      "loss": 8.4873,
      "step": 199
    },
    {
      "epoch": 2.5689655172413794,
      "grad_norm": 3.549838066101074,
      "learning_rate": 4.545454545454545e-05,
      "loss": 7.9704,
      "step": 200
    },
    {
      "epoch": 2.581896551724138,
      "grad_norm": 3.94353985786438,
      "learning_rate": 4.415584415584416e-05,
      "loss": 10.9899,
      "step": 201
    },
    {
      "epoch": 2.594827586206897,
      "grad_norm": 4.8755598068237305,
      "learning_rate": 4.285714285714285e-05,
      "loss": 10.3888,
      "step": 202
    },
    {
      "epoch": 2.6077586206896552,
      "grad_norm": 3.6170802116394043,
      "learning_rate": 4.1558441558441555e-05,
      "loss": 9.2622,
      "step": 203
    },
    {
      "epoch": 2.6206896551724137,
      "grad_norm": 3.6773974895477295,
      "learning_rate": 4.025974025974026e-05,
      "loss": 10.1623,
      "step": 204
    },
    {
      "epoch": 2.6336206896551726,
      "grad_norm": 3.466113805770874,
      "learning_rate": 3.896103896103895e-05,
      "loss": 8.6871,
      "step": 205
    },
    {
      "epoch": 2.646551724137931,
      "grad_norm": 3.3325819969177246,
      "learning_rate": 3.766233766233766e-05,
      "loss": 9.2253,
      "step": 206
    },
    {
      "epoch": 2.6594827586206895,
      "grad_norm": 3.3198912143707275,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 9.1359,
      "step": 207
    },
    {
      "epoch": 2.6724137931034484,
      "grad_norm": 3.3639395236968994,
      "learning_rate": 3.506493506493506e-05,
      "loss": 9.2723,
      "step": 208
    },
    {
      "epoch": 2.685344827586207,
      "grad_norm": 3.197011709213257,
      "learning_rate": 3.376623376623376e-05,
      "loss": 8.8341,
      "step": 209
    },
    {
      "epoch": 2.6982758620689653,
      "grad_norm": 3.4542179107666016,
      "learning_rate": 3.246753246753247e-05,
      "loss": 9.9156,
      "step": 210
    },
    {
      "epoch": 2.711206896551724,
      "grad_norm": 3.3392369747161865,
      "learning_rate": 3.1168831168831166e-05,
      "loss": 9.1776,
      "step": 211
    },
    {
      "epoch": 2.7241379310344827,
      "grad_norm": 3.2692785263061523,
      "learning_rate": 2.987012987012987e-05,
      "loss": 9.8558,
      "step": 212
    },
    {
      "epoch": 2.737068965517241,
      "grad_norm": 3.393235445022583,
      "learning_rate": 2.8571428571428567e-05,
      "loss": 8.7241,
      "step": 213
    },
    {
      "epoch": 2.75,
      "grad_norm": 3.5181381702423096,
      "learning_rate": 2.727272727272727e-05,
      "loss": 9.4652,
      "step": 214
    },
    {
      "epoch": 2.762931034482759,
      "grad_norm": 3.2716124057769775,
      "learning_rate": 2.5974025974025972e-05,
      "loss": 9.7676,
      "step": 215
    },
    {
      "epoch": 2.7758620689655173,
      "grad_norm": 3.5421550273895264,
      "learning_rate": 2.4675324675324674e-05,
      "loss": 9.5031,
      "step": 216
    },
    {
      "epoch": 2.788793103448276,
      "grad_norm": 3.3714680671691895,
      "learning_rate": 2.3376623376623373e-05,
      "loss": 8.1088,
      "step": 217
    },
    {
      "epoch": 2.8017241379310347,
      "grad_norm": 3.1404805183410645,
      "learning_rate": 2.207792207792208e-05,
      "loss": 6.725,
      "step": 218
    },
    {
      "epoch": 2.814655172413793,
      "grad_norm": 3.275380849838257,
      "learning_rate": 2.0779220779220778e-05,
      "loss": 9.5703,
      "step": 219
    },
    {
      "epoch": 2.8275862068965516,
      "grad_norm": 3.485842704772949,
      "learning_rate": 1.9480519480519476e-05,
      "loss": 8.3852,
      "step": 220
    },
    {
      "epoch": 2.8405172413793105,
      "grad_norm": 3.481464385986328,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 9.7771,
      "step": 221
    },
    {
      "epoch": 2.853448275862069,
      "grad_norm": 3.686915159225464,
      "learning_rate": 1.688311688311688e-05,
      "loss": 8.78,
      "step": 222
    },
    {
      "epoch": 2.8663793103448274,
      "grad_norm": 3.2508344650268555,
      "learning_rate": 1.5584415584415583e-05,
      "loss": 9.3999,
      "step": 223
    },
    {
      "epoch": 2.8793103448275863,
      "grad_norm": 3.7216315269470215,
      "learning_rate": 1.4285714285714284e-05,
      "loss": 8.746,
      "step": 224
    },
    {
      "epoch": 2.8922413793103448,
      "grad_norm": 3.4589998722076416,
      "learning_rate": 1.2987012987012986e-05,
      "loss": 8.8634,
      "step": 225
    },
    {
      "epoch": 2.905172413793103,
      "grad_norm": 3.8157830238342285,
      "learning_rate": 1.1688311688311687e-05,
      "loss": 10.6505,
      "step": 226
    },
    {
      "epoch": 2.918103448275862,
      "grad_norm": 2.8838791847229004,
      "learning_rate": 1.0389610389610389e-05,
      "loss": 9.2182,
      "step": 227
    },
    {
      "epoch": 2.9310344827586206,
      "grad_norm": 3.3145503997802734,
      "learning_rate": 9.090909090909091e-06,
      "loss": 9.1247,
      "step": 228
    },
    {
      "epoch": 2.9439655172413794,
      "grad_norm": 3.9006829261779785,
      "learning_rate": 7.792207792207792e-06,
      "loss": 7.66,
      "step": 229
    },
    {
      "epoch": 2.956896551724138,
      "grad_norm": 3.6180737018585205,
      "learning_rate": 6.493506493506493e-06,
      "loss": 9.401,
      "step": 230
    },
    {
      "epoch": 2.969827586206897,
      "grad_norm": 3.5194435119628906,
      "learning_rate": 5.194805194805194e-06,
      "loss": 9.9632,
      "step": 231
    },
    {
      "epoch": 2.969827586206897,
      "eval_loss": 3.130455255508423,
      "eval_runtime": 121.5267,
      "eval_samples_per_second": 0.214,
      "eval_steps_per_second": 0.214,
      "step": 231
    }
  ],
  "logging_steps": 1,
  "max_steps": 231,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.783239204333158e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
