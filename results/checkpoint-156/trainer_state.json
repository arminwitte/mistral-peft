{
  "best_metric": 3.2275547981262207,
  "best_model_checkpoint": "./results/checkpoint-156",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 156,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01293103448275862,
      "grad_norm": 10.52786636352539,
      "learning_rate": 0.0002996103896103896,
      "loss": 31.1942,
      "step": 1
    },
    {
      "epoch": 0.02586206896551724,
      "grad_norm": Infinity,
      "learning_rate": 0.0002996103896103896,
      "loss": 29.7186,
      "step": 2
    },
    {
      "epoch": 0.03879310344827586,
      "grad_norm": 15.216208457946777,
      "learning_rate": 0.0002992207792207792,
      "loss": 29.4036,
      "step": 3
    },
    {
      "epoch": 0.05172413793103448,
      "grad_norm": Infinity,
      "learning_rate": 0.0002992207792207792,
      "loss": 25.5906,
      "step": 4
    },
    {
      "epoch": 0.06465517241379311,
      "grad_norm": 22.137861251831055,
      "learning_rate": 0.0002988311688311688,
      "loss": 26.4487,
      "step": 5
    },
    {
      "epoch": 0.07758620689655173,
      "grad_norm": 19.885671615600586,
      "learning_rate": 0.0002984415584415584,
      "loss": 21.6507,
      "step": 6
    },
    {
      "epoch": 0.09051724137931035,
      "grad_norm": 8.802096366882324,
      "learning_rate": 0.00029805194805194806,
      "loss": 17.4021,
      "step": 7
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 20.607948303222656,
      "learning_rate": 0.0002976623376623376,
      "loss": 18.2423,
      "step": 8
    },
    {
      "epoch": 0.11637931034482758,
      "grad_norm": 28.350313186645508,
      "learning_rate": 0.00029727272727272724,
      "loss": 18.2714,
      "step": 9
    },
    {
      "epoch": 0.12931034482758622,
      "grad_norm": Infinity,
      "learning_rate": 0.00029727272727272724,
      "loss": 13.8875,
      "step": 10
    },
    {
      "epoch": 0.14224137931034483,
      "grad_norm": 9.201376914978027,
      "learning_rate": 0.00029688311688311686,
      "loss": 16.7175,
      "step": 11
    },
    {
      "epoch": 0.15517241379310345,
      "grad_norm": 5.656374931335449,
      "learning_rate": 0.0002964935064935065,
      "loss": 15.3352,
      "step": 12
    },
    {
      "epoch": 0.16810344827586207,
      "grad_norm": 4.303947925567627,
      "learning_rate": 0.0002961038961038961,
      "loss": 15.5346,
      "step": 13
    },
    {
      "epoch": 0.1810344827586207,
      "grad_norm": 4.255771636962891,
      "learning_rate": 0.0002957142857142857,
      "loss": 14.7376,
      "step": 14
    },
    {
      "epoch": 0.1939655172413793,
      "grad_norm": 4.5519866943359375,
      "learning_rate": 0.00029532467532467527,
      "loss": 14.4587,
      "step": 15
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 5.422859191894531,
      "learning_rate": 0.00029493506493506494,
      "loss": 14.9051,
      "step": 16
    },
    {
      "epoch": 0.21982758620689655,
      "grad_norm": 3.9149460792541504,
      "learning_rate": 0.0002945454545454545,
      "loss": 13.8605,
      "step": 17
    },
    {
      "epoch": 0.23275862068965517,
      "grad_norm": 4.530887126922607,
      "learning_rate": 0.0002941558441558441,
      "loss": 13.7279,
      "step": 18
    },
    {
      "epoch": 0.24568965517241378,
      "grad_norm": 4.516247749328613,
      "learning_rate": 0.00029376623376623374,
      "loss": 13.2462,
      "step": 19
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 3.0494680404663086,
      "learning_rate": 0.00029337662337662336,
      "loss": 12.6892,
      "step": 20
    },
    {
      "epoch": 0.27155172413793105,
      "grad_norm": 2.865143060684204,
      "learning_rate": 0.000292987012987013,
      "loss": 11.7024,
      "step": 21
    },
    {
      "epoch": 0.28448275862068967,
      "grad_norm": 2.851221799850464,
      "learning_rate": 0.0002925974025974026,
      "loss": 13.4156,
      "step": 22
    },
    {
      "epoch": 0.2974137931034483,
      "grad_norm": 2.996565103530884,
      "learning_rate": 0.00029220779220779215,
      "loss": 12.6355,
      "step": 23
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 3.519613742828369,
      "learning_rate": 0.0002918181818181818,
      "loss": 12.9158,
      "step": 24
    },
    {
      "epoch": 0.3232758620689655,
      "grad_norm": 3.5722877979278564,
      "learning_rate": 0.0002914285714285714,
      "loss": 12.801,
      "step": 25
    },
    {
      "epoch": 0.33620689655172414,
      "grad_norm": 3.1401360034942627,
      "learning_rate": 0.000291038961038961,
      "loss": 11.9407,
      "step": 26
    },
    {
      "epoch": 0.34913793103448276,
      "grad_norm": 3.809264659881592,
      "learning_rate": 0.0002906493506493506,
      "loss": 12.5599,
      "step": 27
    },
    {
      "epoch": 0.3620689655172414,
      "grad_norm": 2.736335515975952,
      "learning_rate": 0.00029025974025974024,
      "loss": 13.8792,
      "step": 28
    },
    {
      "epoch": 0.375,
      "grad_norm": 2.472539186477661,
      "learning_rate": 0.00028987012987012986,
      "loss": 11.7857,
      "step": 29
    },
    {
      "epoch": 0.3879310344827586,
      "grad_norm": 2.661357879638672,
      "learning_rate": 0.0002894805194805195,
      "loss": 12.7643,
      "step": 30
    },
    {
      "epoch": 0.40086206896551724,
      "grad_norm": 5.460338592529297,
      "learning_rate": 0.00028909090909090904,
      "loss": 10.313,
      "step": 31
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 2.9243104457855225,
      "learning_rate": 0.0002887012987012987,
      "loss": 12.0266,
      "step": 32
    },
    {
      "epoch": 0.4267241379310345,
      "grad_norm": 3.4328372478485107,
      "learning_rate": 0.00028831168831168827,
      "loss": 11.559,
      "step": 33
    },
    {
      "epoch": 0.4396551724137931,
      "grad_norm": 2.1811130046844482,
      "learning_rate": 0.0002879220779220779,
      "loss": 8.3641,
      "step": 34
    },
    {
      "epoch": 0.4525862068965517,
      "grad_norm": 2.9052984714508057,
      "learning_rate": 0.0002875324675324675,
      "loss": 8.9304,
      "step": 35
    },
    {
      "epoch": 0.46551724137931033,
      "grad_norm": 3.070247173309326,
      "learning_rate": 0.0002871428571428571,
      "loss": 11.9554,
      "step": 36
    },
    {
      "epoch": 0.47844827586206895,
      "grad_norm": 2.6084156036376953,
      "learning_rate": 0.00028675324675324674,
      "loss": 12.2419,
      "step": 37
    },
    {
      "epoch": 0.49137931034482757,
      "grad_norm": 2.4284214973449707,
      "learning_rate": 0.00028636363636363636,
      "loss": 11.128,
      "step": 38
    },
    {
      "epoch": 0.5043103448275862,
      "grad_norm": 2.564937114715576,
      "learning_rate": 0.0002859740259740259,
      "loss": 11.6417,
      "step": 39
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 2.346163034439087,
      "learning_rate": 0.0002855844155844156,
      "loss": 11.224,
      "step": 40
    },
    {
      "epoch": 0.5301724137931034,
      "grad_norm": 2.185553789138794,
      "learning_rate": 0.00028519480519480515,
      "loss": 11.3258,
      "step": 41
    },
    {
      "epoch": 0.5431034482758621,
      "grad_norm": 2.2821733951568604,
      "learning_rate": 0.00028480519480519477,
      "loss": 10.7209,
      "step": 42
    },
    {
      "epoch": 0.5560344827586207,
      "grad_norm": 3.0881078243255615,
      "learning_rate": 0.0002844155844155844,
      "loss": 11.0837,
      "step": 43
    },
    {
      "epoch": 0.5689655172413793,
      "grad_norm": 2.2615833282470703,
      "learning_rate": 0.000284025974025974,
      "loss": 11.2119,
      "step": 44
    },
    {
      "epoch": 0.5818965517241379,
      "grad_norm": 2.2390947341918945,
      "learning_rate": 0.0002836363636363636,
      "loss": 10.0825,
      "step": 45
    },
    {
      "epoch": 0.5948275862068966,
      "grad_norm": 4.841338157653809,
      "learning_rate": 0.00028324675324675324,
      "loss": 12.2541,
      "step": 46
    },
    {
      "epoch": 0.6077586206896551,
      "grad_norm": 2.493642807006836,
      "learning_rate": 0.0002828571428571428,
      "loss": 10.9321,
      "step": 47
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 2.06241774559021,
      "learning_rate": 0.00028246753246753247,
      "loss": 11.0235,
      "step": 48
    },
    {
      "epoch": 0.6336206896551724,
      "grad_norm": 2.1756207942962646,
      "learning_rate": 0.00028207792207792204,
      "loss": 11.0328,
      "step": 49
    },
    {
      "epoch": 0.646551724137931,
      "grad_norm": 2.07987117767334,
      "learning_rate": 0.00028168831168831165,
      "loss": 10.5035,
      "step": 50
    },
    {
      "epoch": 0.6594827586206896,
      "grad_norm": 2.004826068878174,
      "learning_rate": 0.00028129870129870127,
      "loss": 10.5305,
      "step": 51
    },
    {
      "epoch": 0.6724137931034483,
      "grad_norm": 2.28597092628479,
      "learning_rate": 0.0002809090909090909,
      "loss": 10.1007,
      "step": 52
    },
    {
      "epoch": 0.6853448275862069,
      "grad_norm": 2.568584680557251,
      "learning_rate": 0.0002805194805194805,
      "loss": 10.5574,
      "step": 53
    },
    {
      "epoch": 0.6982758620689655,
      "grad_norm": 2.085090398788452,
      "learning_rate": 0.0002801298701298701,
      "loss": 9.9017,
      "step": 54
    },
    {
      "epoch": 0.7112068965517241,
      "grad_norm": 1.6893863677978516,
      "learning_rate": 0.0002797402597402597,
      "loss": 7.0776,
      "step": 55
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 2.4079158306121826,
      "learning_rate": 0.00027935064935064936,
      "loss": 11.3421,
      "step": 56
    },
    {
      "epoch": 0.7370689655172413,
      "grad_norm": 2.2062482833862305,
      "learning_rate": 0.0002789610389610389,
      "loss": 10.094,
      "step": 57
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.2670376300811768,
      "learning_rate": 0.00027857142857142854,
      "loss": 10.2068,
      "step": 58
    },
    {
      "epoch": 0.7629310344827587,
      "grad_norm": 2.1744155883789062,
      "learning_rate": 0.00027818181818181815,
      "loss": 10.7044,
      "step": 59
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 2.3357224464416504,
      "learning_rate": 0.00027779220779220777,
      "loss": 10.275,
      "step": 60
    },
    {
      "epoch": 0.7887931034482759,
      "grad_norm": 2.193142890930176,
      "learning_rate": 0.0002774025974025974,
      "loss": 11.1789,
      "step": 61
    },
    {
      "epoch": 0.8017241379310345,
      "grad_norm": 1.7950351238250732,
      "learning_rate": 0.000277012987012987,
      "loss": 10.0453,
      "step": 62
    },
    {
      "epoch": 0.8146551724137931,
      "grad_norm": 4.023983478546143,
      "learning_rate": 0.00027662337662337657,
      "loss": 10.7879,
      "step": 63
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 1.7904735803604126,
      "learning_rate": 0.00027623376623376624,
      "loss": 9.166,
      "step": 64
    },
    {
      "epoch": 0.8405172413793104,
      "grad_norm": 2.1650311946868896,
      "learning_rate": 0.0002758441558441558,
      "loss": 10.5572,
      "step": 65
    },
    {
      "epoch": 0.853448275862069,
      "grad_norm": 2.3350918292999268,
      "learning_rate": 0.0002754545454545454,
      "loss": 10.7565,
      "step": 66
    },
    {
      "epoch": 0.8663793103448276,
      "grad_norm": 2.0237276554107666,
      "learning_rate": 0.00027506493506493504,
      "loss": 8.7012,
      "step": 67
    },
    {
      "epoch": 0.8793103448275862,
      "grad_norm": 1.9923415184020996,
      "learning_rate": 0.00027467532467532465,
      "loss": 9.0953,
      "step": 68
    },
    {
      "epoch": 0.8922413793103449,
      "grad_norm": 1.8598331212997437,
      "learning_rate": 0.00027428571428571427,
      "loss": 9.4066,
      "step": 69
    },
    {
      "epoch": 0.9051724137931034,
      "grad_norm": 1.916157841682434,
      "learning_rate": 0.0002738961038961039,
      "loss": 10.9153,
      "step": 70
    },
    {
      "epoch": 0.9181034482758621,
      "grad_norm": 2.005152702331543,
      "learning_rate": 0.00027350649350649345,
      "loss": 9.077,
      "step": 71
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 2.041991949081421,
      "learning_rate": 0.0002731168831168831,
      "loss": 9.8551,
      "step": 72
    },
    {
      "epoch": 0.9439655172413793,
      "grad_norm": 3.127934694290161,
      "learning_rate": 0.0002727272727272727,
      "loss": 11.1924,
      "step": 73
    },
    {
      "epoch": 0.9568965517241379,
      "grad_norm": 2.5323374271392822,
      "learning_rate": 0.0002723376623376623,
      "loss": 7.9981,
      "step": 74
    },
    {
      "epoch": 0.9698275862068966,
      "grad_norm": 2.2056057453155518,
      "learning_rate": 0.0002719480519480519,
      "loss": 11.5156,
      "step": 75
    },
    {
      "epoch": 0.9827586206896551,
      "grad_norm": 2.343374490737915,
      "learning_rate": 0.00027155844155844154,
      "loss": 10.841,
      "step": 76
    },
    {
      "epoch": 0.9956896551724138,
      "grad_norm": 1.8999567031860352,
      "learning_rate": 0.00027116883116883115,
      "loss": 10.8398,
      "step": 77
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.1000229120254517,
      "learning_rate": 0.00027077922077922077,
      "loss": 4.3106,
      "step": 78
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.446894407272339,
      "eval_runtime": 120.1328,
      "eval_samples_per_second": 0.216,
      "eval_steps_per_second": 0.216,
      "step": 78
    },
    {
      "epoch": 1.0129310344827587,
      "grad_norm": 2.217813014984131,
      "learning_rate": 0.00027038961038961033,
      "loss": 8.3834,
      "step": 79
    },
    {
      "epoch": 1.0258620689655173,
      "grad_norm": 2.360917806625366,
      "learning_rate": 0.00027,
      "loss": 10.23,
      "step": 80
    },
    {
      "epoch": 1.0387931034482758,
      "grad_norm": 1.8772858381271362,
      "learning_rate": 0.00026961038961038957,
      "loss": 10.059,
      "step": 81
    },
    {
      "epoch": 1.0517241379310345,
      "grad_norm": 1.9327290058135986,
      "learning_rate": 0.0002692207792207792,
      "loss": 9.6777,
      "step": 82
    },
    {
      "epoch": 1.0646551724137931,
      "grad_norm": 3.133399248123169,
      "learning_rate": 0.0002688311688311688,
      "loss": 11.2606,
      "step": 83
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 2.5116324424743652,
      "learning_rate": 0.0002684415584415584,
      "loss": 8.9637,
      "step": 84
    },
    {
      "epoch": 1.0905172413793103,
      "grad_norm": 2.8679161071777344,
      "learning_rate": 0.00026805194805194803,
      "loss": 8.9609,
      "step": 85
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 2.0783603191375732,
      "learning_rate": 0.00026766233766233765,
      "loss": 9.9666,
      "step": 86
    },
    {
      "epoch": 1.1163793103448276,
      "grad_norm": 2.1442606449127197,
      "learning_rate": 0.0002672727272727272,
      "loss": 9.0891,
      "step": 87
    },
    {
      "epoch": 1.1293103448275863,
      "grad_norm": 2.0824546813964844,
      "learning_rate": 0.0002668831168831169,
      "loss": 9.5135,
      "step": 88
    },
    {
      "epoch": 1.1422413793103448,
      "grad_norm": 2.0706794261932373,
      "learning_rate": 0.00026649350649350645,
      "loss": 10.571,
      "step": 89
    },
    {
      "epoch": 1.1551724137931034,
      "grad_norm": 2.2278857231140137,
      "learning_rate": 0.00026610389610389607,
      "loss": 10.0693,
      "step": 90
    },
    {
      "epoch": 1.168103448275862,
      "grad_norm": 2.0716915130615234,
      "learning_rate": 0.0002657142857142857,
      "loss": 10.1499,
      "step": 91
    },
    {
      "epoch": 1.1810344827586208,
      "grad_norm": 1.4803828001022339,
      "learning_rate": 0.0002653246753246753,
      "loss": 5.5092,
      "step": 92
    },
    {
      "epoch": 1.1939655172413792,
      "grad_norm": 2.00813627243042,
      "learning_rate": 0.0002649350649350649,
      "loss": 9.5804,
      "step": 93
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 1.8973910808563232,
      "learning_rate": 0.00026454545454545453,
      "loss": 9.2143,
      "step": 94
    },
    {
      "epoch": 1.2198275862068966,
      "grad_norm": 2.7117526531219482,
      "learning_rate": 0.0002641558441558441,
      "loss": 10.1028,
      "step": 95
    },
    {
      "epoch": 1.2327586206896552,
      "grad_norm": 1.9797245264053345,
      "learning_rate": 0.00026376623376623377,
      "loss": 9.6343,
      "step": 96
    },
    {
      "epoch": 1.2456896551724137,
      "grad_norm": 2.5809168815612793,
      "learning_rate": 0.00026337662337662333,
      "loss": 9.4147,
      "step": 97
    },
    {
      "epoch": 1.2586206896551724,
      "grad_norm": 2.2011706829071045,
      "learning_rate": 0.00026298701298701295,
      "loss": 9.737,
      "step": 98
    },
    {
      "epoch": 1.271551724137931,
      "grad_norm": 2.1318976879119873,
      "learning_rate": 0.00026259740259740257,
      "loss": 10.219,
      "step": 99
    },
    {
      "epoch": 1.2844827586206897,
      "grad_norm": 1.9278922080993652,
      "learning_rate": 0.0002622077922077922,
      "loss": 10.9099,
      "step": 100
    },
    {
      "epoch": 1.2974137931034484,
      "grad_norm": 1.9190614223480225,
      "learning_rate": 0.0002618181818181818,
      "loss": 9.6663,
      "step": 101
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 1.9313029050827026,
      "learning_rate": 0.0002614285714285714,
      "loss": 7.2481,
      "step": 102
    },
    {
      "epoch": 1.3232758620689655,
      "grad_norm": 9.257596015930176,
      "learning_rate": 0.000261038961038961,
      "loss": 10.1774,
      "step": 103
    },
    {
      "epoch": 1.3362068965517242,
      "grad_norm": 1.9648219347000122,
      "learning_rate": 0.00026064935064935065,
      "loss": 9.1328,
      "step": 104
    },
    {
      "epoch": 1.3491379310344827,
      "grad_norm": 1.9449876546859741,
      "learning_rate": 0.0002602597402597402,
      "loss": 9.6021,
      "step": 105
    },
    {
      "epoch": 1.3620689655172413,
      "grad_norm": 2.1900267601013184,
      "learning_rate": 0.00025987012987012983,
      "loss": 9.4159,
      "step": 106
    },
    {
      "epoch": 1.375,
      "grad_norm": 2.0220816135406494,
      "learning_rate": 0.00025948051948051945,
      "loss": 9.0619,
      "step": 107
    },
    {
      "epoch": 1.3879310344827587,
      "grad_norm": 2.1737539768218994,
      "learning_rate": 0.00025909090909090907,
      "loss": 10.2018,
      "step": 108
    },
    {
      "epoch": 1.4008620689655173,
      "grad_norm": 2.139909029006958,
      "learning_rate": 0.0002587012987012987,
      "loss": 10.5007,
      "step": 109
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 1.9142653942108154,
      "learning_rate": 0.0002583116883116883,
      "loss": 8.2978,
      "step": 110
    },
    {
      "epoch": 1.4267241379310345,
      "grad_norm": 2.0042953491210938,
      "learning_rate": 0.00025792207792207786,
      "loss": 9.9918,
      "step": 111
    },
    {
      "epoch": 1.4396551724137931,
      "grad_norm": 1.9873871803283691,
      "learning_rate": 0.00025753246753246753,
      "loss": 8.504,
      "step": 112
    },
    {
      "epoch": 1.4525862068965516,
      "grad_norm": 2.142411470413208,
      "learning_rate": 0.0002571428571428571,
      "loss": 9.5578,
      "step": 113
    },
    {
      "epoch": 1.4655172413793103,
      "grad_norm": 2.2078635692596436,
      "learning_rate": 0.0002567532467532467,
      "loss": 9.8619,
      "step": 114
    },
    {
      "epoch": 1.478448275862069,
      "grad_norm": 2.0453217029571533,
      "learning_rate": 0.00025636363636363633,
      "loss": 9.5943,
      "step": 115
    },
    {
      "epoch": 1.4913793103448276,
      "grad_norm": 1.9529404640197754,
      "learning_rate": 0.00025597402597402595,
      "loss": 8.4843,
      "step": 116
    },
    {
      "epoch": 1.5043103448275863,
      "grad_norm": 1.94158136844635,
      "learning_rate": 0.00025558441558441557,
      "loss": 9.5187,
      "step": 117
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 1.995198369026184,
      "learning_rate": 0.0002551948051948052,
      "loss": 7.9668,
      "step": 118
    },
    {
      "epoch": 1.5301724137931034,
      "grad_norm": 1.9523568153381348,
      "learning_rate": 0.00025480519480519475,
      "loss": 6.8613,
      "step": 119
    },
    {
      "epoch": 1.543103448275862,
      "grad_norm": 2.1430697441101074,
      "learning_rate": 0.0002544155844155844,
      "loss": 8.4596,
      "step": 120
    },
    {
      "epoch": 1.5560344827586206,
      "grad_norm": 2.1447532176971436,
      "learning_rate": 0.000254025974025974,
      "loss": 10.293,
      "step": 121
    },
    {
      "epoch": 1.5689655172413794,
      "grad_norm": 2.0703816413879395,
      "learning_rate": 0.0002536363636363636,
      "loss": 9.6516,
      "step": 122
    },
    {
      "epoch": 1.581896551724138,
      "grad_norm": 2.1654164791107178,
      "learning_rate": 0.0002532467532467532,
      "loss": 9.1892,
      "step": 123
    },
    {
      "epoch": 1.5948275862068966,
      "grad_norm": 1.9980101585388184,
      "learning_rate": 0.00025285714285714283,
      "loss": 8.9842,
      "step": 124
    },
    {
      "epoch": 1.6077586206896552,
      "grad_norm": 2.2909698486328125,
      "learning_rate": 0.00025246753246753245,
      "loss": 9.289,
      "step": 125
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 2.280827045440674,
      "learning_rate": 0.00025207792207792207,
      "loss": 9.6007,
      "step": 126
    },
    {
      "epoch": 1.6336206896551724,
      "grad_norm": 1.9891324043273926,
      "learning_rate": 0.00025168831168831163,
      "loss": 10.0584,
      "step": 127
    },
    {
      "epoch": 1.646551724137931,
      "grad_norm": 1.8804330825805664,
      "learning_rate": 0.0002512987012987013,
      "loss": 9.0887,
      "step": 128
    },
    {
      "epoch": 1.6594827586206895,
      "grad_norm": 2.1968653202056885,
      "learning_rate": 0.00025090909090909086,
      "loss": 9.7437,
      "step": 129
    },
    {
      "epoch": 1.6724137931034484,
      "grad_norm": 1.9545689821243286,
      "learning_rate": 0.0002505194805194805,
      "loss": 10.062,
      "step": 130
    },
    {
      "epoch": 1.6853448275862069,
      "grad_norm": 2.1694328784942627,
      "learning_rate": 0.0002501298701298701,
      "loss": 7.704,
      "step": 131
    },
    {
      "epoch": 1.6982758620689655,
      "grad_norm": 2.0891106128692627,
      "learning_rate": 0.0002497402597402597,
      "loss": 9.8371,
      "step": 132
    },
    {
      "epoch": 1.7112068965517242,
      "grad_norm": 2.000087022781372,
      "learning_rate": 0.00024935064935064933,
      "loss": 9.8289,
      "step": 133
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 2.1162772178649902,
      "learning_rate": 0.00024896103896103895,
      "loss": 10.044,
      "step": 134
    },
    {
      "epoch": 1.7370689655172413,
      "grad_norm": 2.0168914794921875,
      "learning_rate": 0.00024857142857142857,
      "loss": 9.8037,
      "step": 135
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.9994523525238037,
      "learning_rate": 0.0002481818181818182,
      "loss": 9.7253,
      "step": 136
    },
    {
      "epoch": 1.7629310344827587,
      "grad_norm": 1.759250283241272,
      "learning_rate": 0.0002477922077922078,
      "loss": 6.7564,
      "step": 137
    },
    {
      "epoch": 1.7758620689655173,
      "grad_norm": 1.9333935976028442,
      "learning_rate": 0.00024740259740259736,
      "loss": 8.6386,
      "step": 138
    },
    {
      "epoch": 1.7887931034482758,
      "grad_norm": 1.9978429079055786,
      "learning_rate": 0.00024701298701298703,
      "loss": 9.0655,
      "step": 139
    },
    {
      "epoch": 1.8017241379310345,
      "grad_norm": 1.9667117595672607,
      "learning_rate": 0.0002466233766233766,
      "loss": 9.403,
      "step": 140
    },
    {
      "epoch": 1.8146551724137931,
      "grad_norm": 1.8939476013183594,
      "learning_rate": 0.0002462337662337662,
      "loss": 8.4356,
      "step": 141
    },
    {
      "epoch": 1.8275862068965516,
      "grad_norm": 1.9339033365249634,
      "learning_rate": 0.00024584415584415583,
      "loss": 8.0405,
      "step": 142
    },
    {
      "epoch": 1.8405172413793105,
      "grad_norm": 1.8857238292694092,
      "learning_rate": 0.00024545454545454545,
      "loss": 9.3614,
      "step": 143
    },
    {
      "epoch": 1.853448275862069,
      "grad_norm": 1.8145661354064941,
      "learning_rate": 0.00024506493506493506,
      "loss": 8.1909,
      "step": 144
    },
    {
      "epoch": 1.8663793103448276,
      "grad_norm": 2.07389235496521,
      "learning_rate": 0.0002446753246753247,
      "loss": 9.025,
      "step": 145
    },
    {
      "epoch": 1.8793103448275863,
      "grad_norm": 1.6738969087600708,
      "learning_rate": 0.00024428571428571424,
      "loss": 6.9063,
      "step": 146
    },
    {
      "epoch": 1.8922413793103448,
      "grad_norm": 2.0785343647003174,
      "learning_rate": 0.0002438961038961039,
      "loss": 9.3818,
      "step": 147
    },
    {
      "epoch": 1.9051724137931034,
      "grad_norm": 2.024149179458618,
      "learning_rate": 0.00024350649350649348,
      "loss": 9.2705,
      "step": 148
    },
    {
      "epoch": 1.918103448275862,
      "grad_norm": 1.6084012985229492,
      "learning_rate": 0.00024311688311688312,
      "loss": 7.7897,
      "step": 149
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 1.8640578985214233,
      "learning_rate": 0.0002427272727272727,
      "loss": 9.3355,
      "step": 150
    },
    {
      "epoch": 1.9439655172413794,
      "grad_norm": 1.8710906505584717,
      "learning_rate": 0.00024233766233766233,
      "loss": 8.1733,
      "step": 151
    },
    {
      "epoch": 1.956896551724138,
      "grad_norm": 2.0483758449554443,
      "learning_rate": 0.00024194805194805192,
      "loss": 8.9717,
      "step": 152
    },
    {
      "epoch": 1.9698275862068966,
      "grad_norm": 2.098539113998413,
      "learning_rate": 0.00024155844155844154,
      "loss": 9.9519,
      "step": 153
    },
    {
      "epoch": 1.9827586206896552,
      "grad_norm": 2.0035452842712402,
      "learning_rate": 0.00024116883116883115,
      "loss": 9.3333,
      "step": 154
    },
    {
      "epoch": 1.9956896551724137,
      "grad_norm": 2.168750524520874,
      "learning_rate": 0.00024077922077922077,
      "loss": 9.0569,
      "step": 155
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9269738793373108,
      "learning_rate": 0.00024038961038961036,
      "loss": 2.261,
      "step": 156
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.2275547981262207,
      "eval_runtime": 120.1171,
      "eval_samples_per_second": 0.216,
      "eval_steps_per_second": 0.216,
      "step": 156
    }
  ],
  "logging_steps": 1,
  "max_steps": 770,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.242017797911347e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
