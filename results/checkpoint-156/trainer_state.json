{
  "best_metric": 3.174562931060791,
  "best_model_checkpoint": "./results/checkpoint-156",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 156,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01293103448275862,
      "grad_norm": NaN,
      "learning_rate": 0.0003,
      "loss": 35.1608,
      "step": 1
    },
    {
      "epoch": 0.02586206896551724,
      "grad_norm": 9.498631477355957,
      "learning_rate": 0.0002987012987012987,
      "loss": 31.9126,
      "step": 2
    },
    {
      "epoch": 0.03879310344827586,
      "grad_norm": Infinity,
      "learning_rate": 0.0002987012987012987,
      "loss": 30.7244,
      "step": 3
    },
    {
      "epoch": 0.05172413793103448,
      "grad_norm": 10.979710578918457,
      "learning_rate": 0.0002974025974025974,
      "loss": 29.3,
      "step": 4
    },
    {
      "epoch": 0.06465517241379311,
      "grad_norm": 20.201818466186523,
      "learning_rate": 0.0002961038961038961,
      "loss": 25.5387,
      "step": 5
    },
    {
      "epoch": 0.07758620689655173,
      "grad_norm": Infinity,
      "learning_rate": 0.0002961038961038961,
      "loss": 26.5997,
      "step": 6
    },
    {
      "epoch": 0.09051724137931035,
      "grad_norm": Infinity,
      "learning_rate": 0.0002961038961038961,
      "loss": 30.392,
      "step": 7
    },
    {
      "epoch": 0.10344827586206896,
      "grad_norm": 40.31182861328125,
      "learning_rate": 0.0002948051948051948,
      "loss": 27.9939,
      "step": 8
    },
    {
      "epoch": 0.11637931034482758,
      "grad_norm": 26.062610626220703,
      "learning_rate": 0.00029350649350649345,
      "loss": 21.8951,
      "step": 9
    },
    {
      "epoch": 0.12931034482758622,
      "grad_norm": 20.186119079589844,
      "learning_rate": 0.00029220779220779215,
      "loss": 19.3927,
      "step": 10
    },
    {
      "epoch": 0.14224137931034483,
      "grad_norm": 12.654959678649902,
      "learning_rate": 0.0002909090909090909,
      "loss": 17.9054,
      "step": 11
    },
    {
      "epoch": 0.15517241379310345,
      "grad_norm": 24.66238784790039,
      "learning_rate": 0.00028961038961038956,
      "loss": 17.7884,
      "step": 12
    },
    {
      "epoch": 0.16810344827586207,
      "grad_norm": 14.557443618774414,
      "learning_rate": 0.00028831168831168827,
      "loss": 17.2697,
      "step": 13
    },
    {
      "epoch": 0.1810344827586207,
      "grad_norm": 44.95820236206055,
      "learning_rate": 0.000287012987012987,
      "loss": 13.6747,
      "step": 14
    },
    {
      "epoch": 0.1939655172413793,
      "grad_norm": 14.186003684997559,
      "learning_rate": 0.0002857142857142857,
      "loss": 16.066,
      "step": 15
    },
    {
      "epoch": 0.20689655172413793,
      "grad_norm": 8.028042793273926,
      "learning_rate": 0.0002844155844155844,
      "loss": 12.5398,
      "step": 16
    },
    {
      "epoch": 0.21982758620689655,
      "grad_norm": 6.74392032623291,
      "learning_rate": 0.0002831168831168831,
      "loss": 15.262,
      "step": 17
    },
    {
      "epoch": 0.23275862068965517,
      "grad_norm": 6.337515354156494,
      "learning_rate": 0.0002818181818181818,
      "loss": 15.2952,
      "step": 18
    },
    {
      "epoch": 0.24568965517241378,
      "grad_norm": 6.145899295806885,
      "learning_rate": 0.0002805194805194805,
      "loss": 14.9373,
      "step": 19
    },
    {
      "epoch": 0.25862068965517243,
      "grad_norm": 8.621206283569336,
      "learning_rate": 0.0002792207792207792,
      "loss": 14.8593,
      "step": 20
    },
    {
      "epoch": 0.27155172413793105,
      "grad_norm": 5.0739827156066895,
      "learning_rate": 0.0002779220779220779,
      "loss": 14.6853,
      "step": 21
    },
    {
      "epoch": 0.28448275862068967,
      "grad_norm": 3.915724754333496,
      "learning_rate": 0.00027662337662337657,
      "loss": 14.5162,
      "step": 22
    },
    {
      "epoch": 0.2974137931034483,
      "grad_norm": 4.684821605682373,
      "learning_rate": 0.0002753246753246753,
      "loss": 14.7288,
      "step": 23
    },
    {
      "epoch": 0.3103448275862069,
      "grad_norm": 4.584661960601807,
      "learning_rate": 0.00027402597402597403,
      "loss": 13.4752,
      "step": 24
    },
    {
      "epoch": 0.3232758620689655,
      "grad_norm": 4.488706588745117,
      "learning_rate": 0.0002727272727272727,
      "loss": 14.558,
      "step": 25
    },
    {
      "epoch": 0.33620689655172414,
      "grad_norm": 3.4738681316375732,
      "learning_rate": 0.0002714285714285714,
      "loss": 13.0702,
      "step": 26
    },
    {
      "epoch": 0.34913793103448276,
      "grad_norm": 4.489497184753418,
      "learning_rate": 0.0002701298701298701,
      "loss": 13.2133,
      "step": 27
    },
    {
      "epoch": 0.3620689655172414,
      "grad_norm": 3.348630428314209,
      "learning_rate": 0.0002688311688311688,
      "loss": 13.708,
      "step": 28
    },
    {
      "epoch": 0.375,
      "grad_norm": 3.9648547172546387,
      "learning_rate": 0.0002675324675324675,
      "loss": 12.8631,
      "step": 29
    },
    {
      "epoch": 0.3879310344827586,
      "grad_norm": 4.0243120193481445,
      "learning_rate": 0.0002662337662337662,
      "loss": 12.6698,
      "step": 30
    },
    {
      "epoch": 0.40086206896551724,
      "grad_norm": 2.8896636962890625,
      "learning_rate": 0.0002649350649350649,
      "loss": 12.3001,
      "step": 31
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 3.873629570007324,
      "learning_rate": 0.0002636363636363636,
      "loss": 12.6123,
      "step": 32
    },
    {
      "epoch": 0.4267241379310345,
      "grad_norm": 5.52213716506958,
      "learning_rate": 0.00026233766233766233,
      "loss": 13.0068,
      "step": 33
    },
    {
      "epoch": 0.4396551724137931,
      "grad_norm": 3.8918497562408447,
      "learning_rate": 0.000261038961038961,
      "loss": 12.7296,
      "step": 34
    },
    {
      "epoch": 0.4525862068965517,
      "grad_norm": 4.151779651641846,
      "learning_rate": 0.00025974025974025974,
      "loss": 12.7894,
      "step": 35
    },
    {
      "epoch": 0.46551724137931033,
      "grad_norm": 4.28049898147583,
      "learning_rate": 0.00025844155844155845,
      "loss": 12.176,
      "step": 36
    },
    {
      "epoch": 0.47844827586206895,
      "grad_norm": 3.344611883163452,
      "learning_rate": 0.0002571428571428571,
      "loss": 9.3734,
      "step": 37
    },
    {
      "epoch": 0.49137931034482757,
      "grad_norm": 4.029280185699463,
      "learning_rate": 0.0002558441558441558,
      "loss": 11.7098,
      "step": 38
    },
    {
      "epoch": 0.5043103448275862,
      "grad_norm": 4.853503227233887,
      "learning_rate": 0.0002545454545454545,
      "loss": 12.6607,
      "step": 39
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 3.959879159927368,
      "learning_rate": 0.0002532467532467532,
      "loss": 11.6397,
      "step": 40
    },
    {
      "epoch": 0.5301724137931034,
      "grad_norm": 4.6632795333862305,
      "learning_rate": 0.0002519480519480519,
      "loss": 12.8616,
      "step": 41
    },
    {
      "epoch": 0.5431034482758621,
      "grad_norm": 4.517310619354248,
      "learning_rate": 0.0002506493506493506,
      "loss": 13.1835,
      "step": 42
    },
    {
      "epoch": 0.5560344827586207,
      "grad_norm": 3.2161476612091064,
      "learning_rate": 0.00024935064935064933,
      "loss": 12.4197,
      "step": 43
    },
    {
      "epoch": 0.5689655172413793,
      "grad_norm": 3.9185047149658203,
      "learning_rate": 0.00024805194805194804,
      "loss": 11.832,
      "step": 44
    },
    {
      "epoch": 0.5818965517241379,
      "grad_norm": 4.3378987312316895,
      "learning_rate": 0.00024675324675324674,
      "loss": 12.0122,
      "step": 45
    },
    {
      "epoch": 0.5948275862068966,
      "grad_norm": 4.143579006195068,
      "learning_rate": 0.00024545454545454545,
      "loss": 12.1129,
      "step": 46
    },
    {
      "epoch": 0.6077586206896551,
      "grad_norm": 3.166975736618042,
      "learning_rate": 0.00024415584415584415,
      "loss": 12.8142,
      "step": 47
    },
    {
      "epoch": 0.6206896551724138,
      "grad_norm": 3.494629383087158,
      "learning_rate": 0.00024285714285714283,
      "loss": 10.8532,
      "step": 48
    },
    {
      "epoch": 0.6336206896551724,
      "grad_norm": 3.337921380996704,
      "learning_rate": 0.00024155844155844154,
      "loss": 11.8737,
      "step": 49
    },
    {
      "epoch": 0.646551724137931,
      "grad_norm": 3.7817816734313965,
      "learning_rate": 0.00024025974025974024,
      "loss": 11.0386,
      "step": 50
    },
    {
      "epoch": 0.6594827586206896,
      "grad_norm": 3.4227616786956787,
      "learning_rate": 0.00023896103896103895,
      "loss": 12.0516,
      "step": 51
    },
    {
      "epoch": 0.6724137931034483,
      "grad_norm": 3.824897050857544,
      "learning_rate": 0.00023766233766233765,
      "loss": 10.0321,
      "step": 52
    },
    {
      "epoch": 0.6853448275862069,
      "grad_norm": 4.330126762390137,
      "learning_rate": 0.00023636363636363633,
      "loss": 12.1112,
      "step": 53
    },
    {
      "epoch": 0.6982758620689655,
      "grad_norm": 2.9986732006073,
      "learning_rate": 0.00023506493506493504,
      "loss": 11.4283,
      "step": 54
    },
    {
      "epoch": 0.7112068965517241,
      "grad_norm": 3.0932910442352295,
      "learning_rate": 0.00023376623376623374,
      "loss": 10.9379,
      "step": 55
    },
    {
      "epoch": 0.7241379310344828,
      "grad_norm": 3.3554084300994873,
      "learning_rate": 0.00023246753246753242,
      "loss": 10.0465,
      "step": 56
    },
    {
      "epoch": 0.7370689655172413,
      "grad_norm": 3.003232479095459,
      "learning_rate": 0.00023116883116883116,
      "loss": 10.3131,
      "step": 57
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.816528081893921,
      "learning_rate": 0.00022987012987012986,
      "loss": 8.3072,
      "step": 58
    },
    {
      "epoch": 0.7629310344827587,
      "grad_norm": 3.3908424377441406,
      "learning_rate": 0.00022857142857142854,
      "loss": 11.476,
      "step": 59
    },
    {
      "epoch": 0.7758620689655172,
      "grad_norm": 3.0751357078552246,
      "learning_rate": 0.00022727272727272725,
      "loss": 9.5416,
      "step": 60
    },
    {
      "epoch": 0.7887931034482759,
      "grad_norm": 4.440732955932617,
      "learning_rate": 0.00022597402597402595,
      "loss": 10.3947,
      "step": 61
    },
    {
      "epoch": 0.8017241379310345,
      "grad_norm": 3.1682794094085693,
      "learning_rate": 0.00022467532467532463,
      "loss": 10.8112,
      "step": 62
    },
    {
      "epoch": 0.8146551724137931,
      "grad_norm": 4.872435092926025,
      "learning_rate": 0.00022337662337662336,
      "loss": 9.4813,
      "step": 63
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 4.583402633666992,
      "learning_rate": 0.00022207792207792207,
      "loss": 10.7395,
      "step": 64
    },
    {
      "epoch": 0.8405172413793104,
      "grad_norm": 2.910429000854492,
      "learning_rate": 0.00022077922077922075,
      "loss": 11.1816,
      "step": 65
    },
    {
      "epoch": 0.853448275862069,
      "grad_norm": 3.464561939239502,
      "learning_rate": 0.00021948051948051945,
      "loss": 9.2225,
      "step": 66
    },
    {
      "epoch": 0.8663793103448276,
      "grad_norm": 3.3045012950897217,
      "learning_rate": 0.00021818181818181816,
      "loss": 10.4823,
      "step": 67
    },
    {
      "epoch": 0.8793103448275862,
      "grad_norm": 3.530181407928467,
      "learning_rate": 0.00021688311688311684,
      "loss": 10.5763,
      "step": 68
    },
    {
      "epoch": 0.8922413793103449,
      "grad_norm": 3.5994646549224854,
      "learning_rate": 0.00021558441558441557,
      "loss": 10.6001,
      "step": 69
    },
    {
      "epoch": 0.9051724137931034,
      "grad_norm": 2.8888847827911377,
      "learning_rate": 0.00021428571428571427,
      "loss": 11.4698,
      "step": 70
    },
    {
      "epoch": 0.9181034482758621,
      "grad_norm": 3.3341593742370605,
      "learning_rate": 0.00021298701298701298,
      "loss": 8.9128,
      "step": 71
    },
    {
      "epoch": 0.9310344827586207,
      "grad_norm": 2.9974467754364014,
      "learning_rate": 0.00021168831168831166,
      "loss": 10.4517,
      "step": 72
    },
    {
      "epoch": 0.9439655172413793,
      "grad_norm": 3.493880033493042,
      "learning_rate": 0.00021038961038961036,
      "loss": 11.2871,
      "step": 73
    },
    {
      "epoch": 0.9568965517241379,
      "grad_norm": 2.766749382019043,
      "learning_rate": 0.0002090909090909091,
      "loss": 9.5786,
      "step": 74
    },
    {
      "epoch": 0.9698275862068966,
      "grad_norm": 3.636368751525879,
      "learning_rate": 0.00020779220779220778,
      "loss": 10.0376,
      "step": 75
    },
    {
      "epoch": 0.9827586206896551,
      "grad_norm": 3.9716851711273193,
      "learning_rate": 0.00020649350649350648,
      "loss": 10.6453,
      "step": 76
    },
    {
      "epoch": 0.9956896551724138,
      "grad_norm": 3.3088743686676025,
      "learning_rate": 0.0002051948051948052,
      "loss": 10.8774,
      "step": 77
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.7290042638778687,
      "learning_rate": 0.00020389610389610387,
      "loss": 3.1406,
      "step": 78
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.3748056888580322,
      "eval_runtime": 121.4753,
      "eval_samples_per_second": 0.214,
      "eval_steps_per_second": 0.214,
      "step": 78
    },
    {
      "epoch": 1.0129310344827587,
      "grad_norm": 3.90639066696167,
      "learning_rate": 0.00020259740259740257,
      "loss": 10.3371,
      "step": 79
    },
    {
      "epoch": 1.0258620689655173,
      "grad_norm": 3.4302334785461426,
      "learning_rate": 0.0002012987012987013,
      "loss": 9.7531,
      "step": 80
    },
    {
      "epoch": 1.0387931034482758,
      "grad_norm": 2.7183854579925537,
      "learning_rate": 0.00019999999999999998,
      "loss": 8.4374,
      "step": 81
    },
    {
      "epoch": 1.0517241379310345,
      "grad_norm": 3.2216830253601074,
      "learning_rate": 0.0001987012987012987,
      "loss": 10.0148,
      "step": 82
    },
    {
      "epoch": 1.0646551724137931,
      "grad_norm": 3.2525274753570557,
      "learning_rate": 0.0001974025974025974,
      "loss": 9.9788,
      "step": 83
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 3.2122344970703125,
      "learning_rate": 0.00019610389610389607,
      "loss": 10.0804,
      "step": 84
    },
    {
      "epoch": 1.0905172413793103,
      "grad_norm": 3.104163646697998,
      "learning_rate": 0.00019480519480519478,
      "loss": 10.3254,
      "step": 85
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 3.3647541999816895,
      "learning_rate": 0.0001935064935064935,
      "loss": 9.3435,
      "step": 86
    },
    {
      "epoch": 1.1163793103448276,
      "grad_norm": 7.6382904052734375,
      "learning_rate": 0.0001922077922077922,
      "loss": 10.2176,
      "step": 87
    },
    {
      "epoch": 1.1293103448275863,
      "grad_norm": 3.9379637241363525,
      "learning_rate": 0.0001909090909090909,
      "loss": 9.7862,
      "step": 88
    },
    {
      "epoch": 1.1422413793103448,
      "grad_norm": 3.733790397644043,
      "learning_rate": 0.0001896103896103896,
      "loss": 10.7702,
      "step": 89
    },
    {
      "epoch": 1.1551724137931034,
      "grad_norm": 2.862022638320923,
      "learning_rate": 0.00018831168831168828,
      "loss": 8.3862,
      "step": 90
    },
    {
      "epoch": 1.168103448275862,
      "grad_norm": 3.198615312576294,
      "learning_rate": 0.00018701298701298698,
      "loss": 10.6849,
      "step": 91
    },
    {
      "epoch": 1.1810344827586208,
      "grad_norm": 3.7742857933044434,
      "learning_rate": 0.00018571428571428572,
      "loss": 9.1489,
      "step": 92
    },
    {
      "epoch": 1.1939655172413792,
      "grad_norm": 3.539418935775757,
      "learning_rate": 0.0001844155844155844,
      "loss": 10.6529,
      "step": 93
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 3.5666630268096924,
      "learning_rate": 0.0001831168831168831,
      "loss": 10.7595,
      "step": 94
    },
    {
      "epoch": 1.2198275862068966,
      "grad_norm": 3.0167009830474854,
      "learning_rate": 0.0001818181818181818,
      "loss": 10.5005,
      "step": 95
    },
    {
      "epoch": 1.2327586206896552,
      "grad_norm": 3.0873451232910156,
      "learning_rate": 0.0001805194805194805,
      "loss": 9.7142,
      "step": 96
    },
    {
      "epoch": 1.2456896551724137,
      "grad_norm": 3.8717472553253174,
      "learning_rate": 0.0001792207792207792,
      "loss": 10.3182,
      "step": 97
    },
    {
      "epoch": 1.2586206896551724,
      "grad_norm": 3.0050768852233887,
      "learning_rate": 0.0001779220779220779,
      "loss": 9.6228,
      "step": 98
    },
    {
      "epoch": 1.271551724137931,
      "grad_norm": 3.7609457969665527,
      "learning_rate": 0.00017662337662337663,
      "loss": 10.6724,
      "step": 99
    },
    {
      "epoch": 1.2844827586206897,
      "grad_norm": 4.418886184692383,
      "learning_rate": 0.0001753246753246753,
      "loss": 10.8386,
      "step": 100
    },
    {
      "epoch": 1.2974137931034484,
      "grad_norm": 3.475203514099121,
      "learning_rate": 0.00017402597402597401,
      "loss": 9.7702,
      "step": 101
    },
    {
      "epoch": 1.3103448275862069,
      "grad_norm": 3.54891037940979,
      "learning_rate": 0.00017272727272727272,
      "loss": 10.902,
      "step": 102
    },
    {
      "epoch": 1.3232758620689655,
      "grad_norm": 3.6132125854492188,
      "learning_rate": 0.0001714285714285714,
      "loss": 7.685,
      "step": 103
    },
    {
      "epoch": 1.3362068965517242,
      "grad_norm": 3.5246353149414062,
      "learning_rate": 0.0001701298701298701,
      "loss": 10.0989,
      "step": 104
    },
    {
      "epoch": 1.3491379310344827,
      "grad_norm": 3.6160497665405273,
      "learning_rate": 0.00016883116883116884,
      "loss": 10.5049,
      "step": 105
    },
    {
      "epoch": 1.3620689655172413,
      "grad_norm": 3.327263593673706,
      "learning_rate": 0.00016753246753246751,
      "loss": 10.0084,
      "step": 106
    },
    {
      "epoch": 1.375,
      "grad_norm": 3.201719284057617,
      "learning_rate": 0.00016623376623376622,
      "loss": 10.3016,
      "step": 107
    },
    {
      "epoch": 1.3879310344827587,
      "grad_norm": 6.096494197845459,
      "learning_rate": 0.00016493506493506493,
      "loss": 10.0649,
      "step": 108
    },
    {
      "epoch": 1.4008620689655173,
      "grad_norm": 3.4147119522094727,
      "learning_rate": 0.0001636363636363636,
      "loss": 8.9458,
      "step": 109
    },
    {
      "epoch": 1.4137931034482758,
      "grad_norm": 3.808351755142212,
      "learning_rate": 0.0001623376623376623,
      "loss": 10.1294,
      "step": 110
    },
    {
      "epoch": 1.4267241379310345,
      "grad_norm": 3.2952849864959717,
      "learning_rate": 0.00016103896103896104,
      "loss": 10.9406,
      "step": 111
    },
    {
      "epoch": 1.4396551724137931,
      "grad_norm": 3.35164475440979,
      "learning_rate": 0.00015974025974025972,
      "loss": 9.6685,
      "step": 112
    },
    {
      "epoch": 1.4525862068965516,
      "grad_norm": 3.716529130935669,
      "learning_rate": 0.00015844155844155843,
      "loss": 9.6007,
      "step": 113
    },
    {
      "epoch": 1.4655172413793103,
      "grad_norm": 3.1733248233795166,
      "learning_rate": 0.00015714285714285713,
      "loss": 9.712,
      "step": 114
    },
    {
      "epoch": 1.478448275862069,
      "grad_norm": 3.396585702896118,
      "learning_rate": 0.0001558441558441558,
      "loss": 10.8524,
      "step": 115
    },
    {
      "epoch": 1.4913793103448276,
      "grad_norm": 3.0955801010131836,
      "learning_rate": 0.00015454545454545452,
      "loss": 8.1923,
      "step": 116
    },
    {
      "epoch": 1.5043103448275863,
      "grad_norm": 2.832077741622925,
      "learning_rate": 0.00015324675324675325,
      "loss": 10.0065,
      "step": 117
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 4.427791118621826,
      "learning_rate": 0.00015194805194805193,
      "loss": 10.303,
      "step": 118
    },
    {
      "epoch": 1.5301724137931034,
      "grad_norm": 4.510751724243164,
      "learning_rate": 0.00015064935064935063,
      "loss": 8.8813,
      "step": 119
    },
    {
      "epoch": 1.543103448275862,
      "grad_norm": 3.508016347885132,
      "learning_rate": 0.00014935064935064934,
      "loss": 10.243,
      "step": 120
    },
    {
      "epoch": 1.5560344827586206,
      "grad_norm": 3.3763723373413086,
      "learning_rate": 0.00014805194805194805,
      "loss": 9.3128,
      "step": 121
    },
    {
      "epoch": 1.5689655172413794,
      "grad_norm": 3.0946271419525146,
      "learning_rate": 0.00014675324675324672,
      "loss": 9.9107,
      "step": 122
    },
    {
      "epoch": 1.581896551724138,
      "grad_norm": 3.0196030139923096,
      "learning_rate": 0.00014545454545454546,
      "loss": 9.0618,
      "step": 123
    },
    {
      "epoch": 1.5948275862068966,
      "grad_norm": 4.014636039733887,
      "learning_rate": 0.00014415584415584414,
      "loss": 10.994,
      "step": 124
    },
    {
      "epoch": 1.6077586206896552,
      "grad_norm": 3.2915942668914795,
      "learning_rate": 0.00014285714285714284,
      "loss": 9.3206,
      "step": 125
    },
    {
      "epoch": 1.6206896551724137,
      "grad_norm": 3.5697362422943115,
      "learning_rate": 0.00014155844155844155,
      "loss": 9.9851,
      "step": 126
    },
    {
      "epoch": 1.6336206896551724,
      "grad_norm": 3.216257333755493,
      "learning_rate": 0.00014025974025974025,
      "loss": 9.4353,
      "step": 127
    },
    {
      "epoch": 1.646551724137931,
      "grad_norm": 3.3360679149627686,
      "learning_rate": 0.00013896103896103896,
      "loss": 9.8191,
      "step": 128
    },
    {
      "epoch": 1.6594827586206895,
      "grad_norm": 3.432391881942749,
      "learning_rate": 0.00013766233766233766,
      "loss": 10.2968,
      "step": 129
    },
    {
      "epoch": 1.6724137931034484,
      "grad_norm": 3.3247623443603516,
      "learning_rate": 0.00013636363636363634,
      "loss": 9.5164,
      "step": 130
    },
    {
      "epoch": 1.6853448275862069,
      "grad_norm": 3.3683533668518066,
      "learning_rate": 0.00013506493506493505,
      "loss": 9.4234,
      "step": 131
    },
    {
      "epoch": 1.6982758620689655,
      "grad_norm": 3.5049667358398438,
      "learning_rate": 0.00013376623376623375,
      "loss": 10.5332,
      "step": 132
    },
    {
      "epoch": 1.7112068965517242,
      "grad_norm": 3.3907434940338135,
      "learning_rate": 0.00013246753246753246,
      "loss": 10.0097,
      "step": 133
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 3.4843459129333496,
      "learning_rate": 0.00013116883116883116,
      "loss": 10.5646,
      "step": 134
    },
    {
      "epoch": 1.7370689655172413,
      "grad_norm": 2.977480888366699,
      "learning_rate": 0.00012987012987012987,
      "loss": 9.2909,
      "step": 135
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.1326963901519775,
      "learning_rate": 0.00012857142857142855,
      "loss": 10.0172,
      "step": 136
    },
    {
      "epoch": 1.7629310344827587,
      "grad_norm": 3.6457619667053223,
      "learning_rate": 0.00012727272727272725,
      "loss": 9.4737,
      "step": 137
    },
    {
      "epoch": 1.7758620689655173,
      "grad_norm": 4.171846389770508,
      "learning_rate": 0.00012597402597402596,
      "loss": 10.604,
      "step": 138
    },
    {
      "epoch": 1.7887931034482758,
      "grad_norm": 4.250638008117676,
      "learning_rate": 0.00012467532467532467,
      "loss": 10.2323,
      "step": 139
    },
    {
      "epoch": 1.8017241379310345,
      "grad_norm": 3.429205894470215,
      "learning_rate": 0.00012337662337662337,
      "loss": 9.9769,
      "step": 140
    },
    {
      "epoch": 1.8146551724137931,
      "grad_norm": 3.4709386825561523,
      "learning_rate": 0.00012207792207792208,
      "loss": 10.3017,
      "step": 141
    },
    {
      "epoch": 1.8275862068965516,
      "grad_norm": 3.180453062057495,
      "learning_rate": 0.00012077922077922077,
      "loss": 7.1831,
      "step": 142
    },
    {
      "epoch": 1.8405172413793105,
      "grad_norm": 3.580900192260742,
      "learning_rate": 0.00011948051948051947,
      "loss": 8.6476,
      "step": 143
    },
    {
      "epoch": 1.853448275862069,
      "grad_norm": 3.4623281955718994,
      "learning_rate": 0.00011818181818181817,
      "loss": 10.3495,
      "step": 144
    },
    {
      "epoch": 1.8663793103448276,
      "grad_norm": 3.208652973175049,
      "learning_rate": 0.00011688311688311687,
      "loss": 8.3731,
      "step": 145
    },
    {
      "epoch": 1.8793103448275863,
      "grad_norm": 3.183626890182495,
      "learning_rate": 0.00011558441558441558,
      "loss": 8.8654,
      "step": 146
    },
    {
      "epoch": 1.8922413793103448,
      "grad_norm": 3.653122663497925,
      "learning_rate": 0.00011428571428571427,
      "loss": 9.1719,
      "step": 147
    },
    {
      "epoch": 1.9051724137931034,
      "grad_norm": 3.40657639503479,
      "learning_rate": 0.00011298701298701298,
      "loss": 10.3183,
      "step": 148
    },
    {
      "epoch": 1.918103448275862,
      "grad_norm": 3.131134510040283,
      "learning_rate": 0.00011168831168831168,
      "loss": 10.4519,
      "step": 149
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 3.2778353691101074,
      "learning_rate": 0.00011038961038961037,
      "loss": 9.204,
      "step": 150
    },
    {
      "epoch": 1.9439655172413794,
      "grad_norm": 3.291884660720825,
      "learning_rate": 0.00010909090909090908,
      "loss": 9.1019,
      "step": 151
    },
    {
      "epoch": 1.956896551724138,
      "grad_norm": 2.986346960067749,
      "learning_rate": 0.00010779220779220778,
      "loss": 7.6359,
      "step": 152
    },
    {
      "epoch": 1.9698275862068966,
      "grad_norm": 2.9058985710144043,
      "learning_rate": 0.00010649350649350649,
      "loss": 7.1165,
      "step": 153
    },
    {
      "epoch": 1.9827586206896552,
      "grad_norm": 4.196706295013428,
      "learning_rate": 0.00010519480519480518,
      "loss": 9.2609,
      "step": 154
    },
    {
      "epoch": 1.9956896551724137,
      "grad_norm": 3.339599847793579,
      "learning_rate": 0.00010389610389610389,
      "loss": 8.6662,
      "step": 155
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.755329966545105,
      "learning_rate": 0.0001025974025974026,
      "loss": 3.4999,
      "step": 156
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.174562931060791,
      "eval_runtime": 121.4693,
      "eval_samples_per_second": 0.214,
      "eval_steps_per_second": 0.214,
      "step": 156
    }
  ],
  "logging_steps": 1,
  "max_steps": 231,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.2212234989993984e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
